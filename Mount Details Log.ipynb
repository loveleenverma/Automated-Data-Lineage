{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f666bdaa-75b3-432b-9b1d-9faa5dcac6c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting gdown\n  Using cached gdown-5.2.0-py3-none-any.whl (18 kB)\nRequirement already satisfied: requests[socks] in /databricks/python3/lib/python3.9/site-packages (from gdown) (2.27.1)\nRequirement already satisfied: beautifulsoup4 in /databricks/python3/lib/python3.9/site-packages (from gdown) (4.11.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from gdown) (3.9.0)\nCollecting tqdm\n  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\nRequirement already satisfied: soupsieve>1.2 in /databricks/python3/lib/python3.9/site-packages (from beautifulsoup4->gdown) (2.3.1)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests[socks]->gdown) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests[socks]->gdown) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests[socks]->gdown) (2021.10.8)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests[socks]->gdown) (1.26.9)\nCollecting PySocks!=1.5.7,>=1.5.6\n  Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\nInstalling collected packages: PySocks, tqdm, gdown\nSuccessfully installed PySocks-1.7.1 gdown-5.2.0 tqdm-4.67.1\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8bbede2b-db49-4fa0-bd5a-dc83ec430a57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import gdown\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "import shutil\n",
    "\n",
    "class ExternalFileLogger:\n",
    "    def __init__(self, log_path=\"/mnt/datalake/Pipeline1/Mount_And_External_Sources/file_write_log/\"):\n",
    "        self.spark = SparkSession.builder.getOrCreate()\n",
    "        self.log_path = log_path\n",
    "\n",
    "    def log_file_event(self, source_type: str, file_path: str):\n",
    "        schema = StructType([\n",
    "        StructField(\"source_type\", StringType(), True),\n",
    "        StructField(\"file_path\", StringType(), True),\n",
    "        StructField(\"timestamp\", StringType(), True)\n",
    "        ])\n",
    "\n",
    "        data = [(source_type, file_path, datetime.now().isoformat())]\n",
    "        log_df = self.spark.createDataFrame(data, schema=schema)\n",
    "\n",
    "        log_df.show()  # Check if the row is there\n",
    "\n",
    "        log_df.write.mode(\"append\").format(\"delta\") \\\n",
    "            .option(\"mergeSchema\", \"true\") \\\n",
    "            .save(self.log_path)\n",
    "        print(f\"Logged: [{source_type}] -> {file_path}\")\n",
    "\n",
    "    def _ensure_directory(self, output_path):\n",
    "        local_dir = os.path.dirname(output_path)\n",
    "        if not os.path.exists(local_dir):\n",
    "            os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "    def download_from_gdrive(self, file_id, output_path):\n",
    "        self._ensure_directory(output_path)\n",
    "        # Download using gdown\n",
    "        source_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "        local_path = gdown.download(source_url, output_path, quiet=False)\n",
    "        # Convert to dbfs path for logging\n",
    "        dbfs_path = local_path.replace(\"/dbfs\", \"dbfs:\")\n",
    "        # Copy file manually to DBFS\n",
    "        shutil.copy(local_path, \"/tmp/my_google_file.csv\")\n",
    "        dbutils.fs.cp(\"file:/tmp/my_google_file.csv\", dbfs_path)\n",
    "        df = spark.read.csv(dbfs_path)\n",
    "        print(df)\n",
    "        print(f\"Downloaded to: {output_path}\")\n",
    "        self.log_file_event(source_url, dbfs_path)\n",
    "\n",
    "    def download_from_sharepoint(self, download_url: str, output_path: str):\n",
    "        response = requests.get(download_url)\n",
    "        with open(output_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        self.log_file_event(\"SharePoint\", output_path)\n",
    "\n",
    "    def log_mount(self, mount_source: str, mount_point: str, extra_configs: dict = None):\n",
    "        if extra_configs:\n",
    "            dbutils.fs.mount(\n",
    "                source=mount_source,\n",
    "                mount_point=mount_point,\n",
    "                extra_configs=extra_configs\n",
    "            )\n",
    "        else:\n",
    "            dbutils.fs.mount(\n",
    "                source=mount_source,\n",
    "                mount_point=mount_point\n",
    "            )\n",
    "        self.log_file_event(mount_source, mount_point)\n",
    "\n",
    "    def show_logs(self):\n",
    "        df = self.spark.read.format(\"delta\").load(self.log_path)\n",
    "        df.orderBy(\"timestamp\", ascending=False).show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Mount Log",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
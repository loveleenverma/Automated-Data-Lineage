{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "02dd009a-b6e3-4e98-968d-f398a89dd9f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries for your ETL pipeline\n",
    "from pyspark.sql import functions as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "19480e2c-e621-462e-860e-d04a0ca45820",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Logging function to capture the start time of pipeline execution\n",
    "def on_execution_start(pipeline_name):\n",
    "    start_time = time.time()  # Capture the start time (timestamp)\n",
    "    print(f\"Pipeline {pipeline_name} started at {start_time}\")\n",
    "    return start_time\n",
    "\n",
    "# Logging function to capture the end time of pipeline execution\n",
    "def on_execution_end(start_time, pipeline_name):\n",
    "    end_time = time.time()  # Capture the end time (timestamp)\n",
    "    execution_duration = end_time - start_time  # Calculate the duration\n",
    "    print(f\"Pipeline {pipeline_name} completed in {execution_duration} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5e4d18c0-b3de-47c4-a42f-bfcf575e9253",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "def set_uuid():\n",
    "    global unique_id\n",
    "    unique_id = uuid.uuid4()\n",
    "    print(str(unique_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79d1e39f-535d-4e47-aab0-770c163af823",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, from_json\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "def combine_files():\n",
    "    # Define the schema for `w.details`\n",
    "    details_schema = StructType([\n",
    "        StructField(\"path\", StringType(), True)  # Assuming `details` contains a JSON with `path`\n",
    "    ])\n",
    "\n",
    "    # Run SQL Query to join all logs\n",
    "    df = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            s.uuid AS `uuid`,\n",
    "            s.source_file AS `source`,\n",
    "            s.file_format,\n",
    "            s.additional_info AS `read_parameters`,\n",
    "            s.timestamp AS `read_timestamp`,\n",
    "            t.transformation_name,\n",
    "            t.details AS `modification_details`,\n",
    "            t.timestamp AS `modification_timestamp`,\n",
    "            w.action_name AS `write_action`,\n",
    "            w.details AS `write_details`,\n",
    "            w.timestamp AS `write_timestamp`     \n",
    "        FROM delta.`/mnt/datalake/Pipeline1/source1/source_logs/` AS s \n",
    "        INNER JOIN delta.`/mnt/datalake/Pipeline1/transformation1/trans_logs/` AS t \n",
    "        ON s.uuid = t.uuid \n",
    "        INNER JOIN delta.`/mnt/datalake/Pipeline1/destination1/write_logs/` AS w \n",
    "        ON s.uuid = w.uuid\n",
    "    \"\"\")\n",
    "\n",
    "    # Convert `w.details` (JSON string) into a struct and extract `path`\n",
    "    df = df.withColumn(\"destination\", from_json(col(\"write_details\"), details_schema).getField(\"path\"))\n",
    "\n",
    "    # Write the transformed data to Delta Lake\n",
    "    df.write.mode(\"append\").option(\"mergeSchema\", \"True\").format(\"delta\").save(\"/mnt/datalake/Pipeline1/lineage/lineage_logs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2258e052-7d92-46d0-a96d-2334fcb4404f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def combine_sql_file():\n",
    "    from delta.tables import DeltaTable\n",
    "    from pyspark.sql.functions import col\n",
    "\n",
    "    # Read new data from SQL logs\n",
    "    new_df = spark.read.format(\"delta\").load(\"/mnt/datalake/Pipeline1/sql/sql_logs/\")\n",
    "\n",
    "    # Define the target path\n",
    "    target_path = \"/mnt/datalake/Pipeline1/lineage/lineage_logs/\"\n",
    "\n",
    "    # Check if the Delta table already exists\n",
    "    if DeltaTable.isDeltaTable(spark, target_path):\n",
    "        # Load the existing Delta table\n",
    "        target_table = DeltaTable.forPath(spark, target_path)\n",
    "\n",
    "        target_table.alias(\"target\").merge(\n",
    "            new_df.alias(\"source\"),\n",
    "            \"target.uuid = source.uuid\" \n",
    "        ).whenNotMatchedInsert(values={\n",
    "            \"source\": col(\"source.source_tables\"),\n",
    "            \"modification_details\": col(\"source.query\"),\n",
    "            \"modification_timestamp\": col(\"source.timestamp\"),\n",
    "            \"destination\": col(\"source.destination_tables\") \n",
    "        }).execute()\n",
    "\n",
    "    else:\n",
    "        # If the table doesn't exist, create it\n",
    "        new_df.write.format(\"delta\").mode(\"overwrite\").save(target_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a348922-3832-4af0-88ef-0693f52a4fe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=1799762579751185#setting/sparkui/0401-084926-jbkg5z6/driver-1027989502902419191\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=1799762579751185#setting/sparkui/0401-084926-jbkg5z6/driver-1027989502902419191\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import DataFrameReader\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "# Global variable to store log directory path\n",
    "logs_dir = \"/mnt/datalake/source_logs/\"  # Default value\n",
    "\n",
    "# Function to set logs directory path dynamically\n",
    "def set_source_logs_dir(path):\n",
    "    global logs_dir\n",
    "    logs_dir = path\n",
    "    print(f\"Logs directory set to: {logs_dir}\")\n",
    "\n",
    "# Wrapper class around the DataFrameReader to intercept file read operations\n",
    "class CustomDataFrameReader(DataFrameReader):\n",
    "    def __init__(self, spark: SparkSession):\n",
    "        super().__init__(spark)\n",
    "        self.spark = spark\n",
    "\n",
    "    def _log_source_details(self, file_path, file_format, additional_info=None):\n",
    "        global logs_dir\n",
    "\n",
    "        # Ensure the logs directory exists (create if not present)\n",
    "        try:\n",
    "            # Check if the directory exists, create if not\n",
    "            if not any(dbutils.fs.ls(logs_dir)):\n",
    "                dbutils.fs.mkdirs(logs_dir)  # Create the directory if it doesn't exist\n",
    "            print(f\"Log directory {logs_dir} is ready.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error while creating directory {logs_dir}: {e}\")\n",
    "        \n",
    "        \n",
    "        # Metadata to log\n",
    "        metadata = {\n",
    "            'uuid': str(unique_id),\n",
    "            'source_file': file_path,\n",
    "            'file_format': file_format,\n",
    "            'additional_info': str(additional_info) if additional_info else '',\n",
    "            'timestamp': str(datetime.now())\n",
    "        }\n",
    "        \n",
    "        # Create a DataFrame for the source log and write it to Delta\n",
    "        lineage_df = self.spark.createDataFrame([metadata])\n",
    "        \n",
    "        try:\n",
    "            lineage_df.write.format(\"delta\").mode(\"append\").save(logs_dir)\n",
    "            print(f\"Source log successfully written for {file_path} to {logs_dir}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error while writing to {logs_dir}: {e}\")\n",
    "\n",
    "    def csv(self, path, **options):\n",
    "        \"\"\"Overriding the CSV read method to include logging\"\"\"\n",
    "        # Log source details\n",
    "        self._log_source_details(path, 'csv', options)\n",
    "        \n",
    "        # Call the original CSV read method\n",
    "        return super().csv(path, **options)\n",
    "\n",
    "    def parquet(self, path, **options):\n",
    "        \"\"\"Overriding the Parquet read method to include logging\"\"\"\n",
    "        # Log source details\n",
    "        self._log_source_details(path, 'parquet', options)\n",
    "        \n",
    "        # Call the original Parquet read method\n",
    "        return super().parquet(path, **options)\n",
    "\n",
    "    def format(self, source_format):\n",
    "        \"\"\"Overriding format method to capture format type\"\"\"\n",
    "        self._current_format = source_format\n",
    "        return super().format(source_format)\n",
    "    \n",
    "    def load(self, path=None, **options):\n",
    "        \"\"\"Overriding load method to log details before reading\"\"\"\n",
    "        file_format = getattr(self, \"_current_format\", \"unknown\")\n",
    "        if path:\n",
    "            self._log_source_details(path, file_format, options)\n",
    "        return super().load(path, **options)\n",
    "\n",
    "# Override the default DataFrameReader with our custom one\n",
    "def wrap_spark_reader(spark: SparkSession):\n",
    "    spark._read = CustomDataFrameReader(spark)\n",
    "    return spark\n",
    "\n",
    "# Initialize the SparkSession and apply the wrapper\n",
    "spark = SparkSession.builder.appName(\"ETL Pipeline\").getOrCreate()\n",
    "wrap_spark_reader(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2ae928e7-cf0e-427b-9e30-b4fea79dc070",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from datetime import datetime\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Global variable to store log directory path\n",
    "transformation_logs_dir = \"/mnt/datalake/trans_logs/\"  # Default value\n",
    "\n",
    "# Function to set logs directory path dynamically\n",
    "def set_trans_logs_dir(path):\n",
    "    global transformation_logs_dir\n",
    "    transformation_logs_dir = path\n",
    "    print(f\"Logs directory set to: {transformation_logs_dir}\")\n",
    "\n",
    "# Function to log transformations\n",
    "# Global variable to track if the directory has been created\n",
    "log_directory_created = False  \n",
    "\n",
    "def log_transformation(transformation_name, details):\n",
    "    global transformation_logs_dir, log_directory_created\n",
    "\n",
    "    if not log_directory_created:\n",
    "        try:\n",
    "            dbutils.fs.ls(transformation_logs_dir)  # Check if directory exists\n",
    "        except Exception:\n",
    "            dbutils.fs.mkdirs(transformation_logs_dir)  # Create if not exists\n",
    "            print(f\"Log directory {transformation_logs_dir} is ready.\")\n",
    "        log_directory_created = True  # Mark as created to avoid repeated checks\n",
    "\n",
    "    metadata = {\n",
    "        'uuid': str(unique_id),\n",
    "        'transformation_name': transformation_name,\n",
    "        'details': str(details),\n",
    "        'timestamp': str(datetime.now())\n",
    "    }\n",
    "    lineage_df = spark.createDataFrame([metadata])\n",
    "    lineage_df.write.format(\"delta\").mode(\"append\").save(transformation_logs_dir)\n",
    "\n",
    "\n",
    "# Class to log and wrap DataFrame transformations\n",
    "class CapturingDataFrame:\n",
    "    def __init__(self, spark_df, pipeline_name):\n",
    "        self._df = spark_df  # Store the original DataFrame\n",
    "        self.pipeline_name = pipeline_name  # Store the pipeline name\n",
    "\n",
    "    def _log_and_apply(self, transformation_name, *args, **kwargs):\n",
    "        \"\"\"Helper function to log and apply transformations.\"\"\"\n",
    "        log_transformation(transformation_name, {\"args\": args, \"kwargs\": kwargs})\n",
    "        return getattr(self._df, transformation_name)(*args, **kwargs)\n",
    "\n",
    "    # Overriding DataFrame transformations\n",
    "    def filter(self, *args, **kwargs):\n",
    "        return CapturingDataFrame(self._log_and_apply(\"filter\", *args, **kwargs), self.pipeline_name)\n",
    "\n",
    "    def groupBy(self, *args, **kwargs):\n",
    "        return CapturingDataFrame(self._log_and_apply(\"groupBy\", *args, **kwargs), self.pipeline_name)\n",
    "\n",
    "    def agg(self, *args, **kwargs):\n",
    "        return CapturingDataFrame(self._log_and_apply(\"agg\", *args, **kwargs), self.pipeline_name)\n",
    "\n",
    "    def withColumnRenamed(self, *args, **kwargs):\n",
    "        return CapturingDataFrame(self._log_and_apply(\"withColumnRenamed\", *args, **kwargs), self.pipeline_name)\n",
    "\n",
    "    def select(self, *args, **kwargs):\n",
    "        return CapturingDataFrame(self._log_and_apply(\"select\", *args, **kwargs), self.pipeline_name)\n",
    "\n",
    "    def join(self, *args, **kwargs):\n",
    "        return CapturingDataFrame(self._log_and_apply(\"join\", *args, **kwargs), self.pipeline_name)\n",
    "\n",
    "    def withColumn(self, *args, **kwargs):\n",
    "        return CapturingDataFrame(self._log_and_apply(\"withColumn\", *args, **kwargs), self.pipeline_name)\n",
    "\n",
    "    def drop(self, *args, **kwargs):\n",
    "        return CapturingDataFrame(self._log_and_apply(\"drop\", *args, **kwargs), self.pipeline_name)\n",
    "\n",
    "    def dropDuplicates(self, *args, **kwargs):\n",
    "        return CapturingDataFrame(self._log_and_apply(\"dropDuplicates\", *args, **kwargs), self.pipeline_name)\n",
    "\n",
    "    def distinct(self, *args, **kwargs):\n",
    "        return CapturingDataFrame(self._log_and_apply(\"distinct\", *args, **kwargs), self.pipeline_name)\n",
    "\n",
    "    def orderBy(self, *args, **kwargs):\n",
    "        return CapturingDataFrame(self._log_and_apply(\"orderBy\", *args, **kwargs), self.pipeline_name)\n",
    "\n",
    "    def limit(self, *args, **kwargs):\n",
    "        return CapturingDataFrame(self._log_and_apply(\"limit\", *args, **kwargs), self.pipeline_name)\n",
    "\n",
    "    def cache(self, *args, **kwargs):\n",
    "        return CapturingDataFrame(self._log_and_apply(\"cache\", *args, **kwargs), self.pipeline_name)\n",
    "\n",
    "    def persist(self, *args, **kwargs):\n",
    "        return CapturingDataFrame(self._log_and_apply(\"persist\", *args, **kwargs), self.pipeline_name)\n",
    "\n",
    "    def repartition(self, *args, **kwargs):\n",
    "        return CapturingDataFrame(self._log_and_apply(\"repartition\", *args, **kwargs), self.pipeline_name)\n",
    "\n",
    "    def coalesce(self, *args, **kwargs):\n",
    "        return CapturingDataFrame(self._log_and_apply(\"coalesce\", *args, **kwargs), self.pipeline_name)\n",
    "\n",
    "    def fillna(self, *args, **kwargs):\n",
    "        return CapturingDataFrame(self._log_and_apply(\"fillna\", *args, **kwargs), self.pipeline_name)\n",
    "\n",
    "    def dropna(self, *args, **kwargs):\n",
    "        return CapturingDataFrame(self._log_and_apply(\"dropna\", *args, **kwargs), self.pipeline_name)\n",
    "\n",
    "    def transform(self, *args, **kwargs):\n",
    "        return CapturingDataFrame(self._log_and_apply(\"transform\", *args, **kwargs), self.pipeline_name)\n",
    "\n",
    "    def alias(self, *args, **kwargs):\n",
    "        return CapturingDataFrame(self._log_and_apply(\"alias\", *args, **kwargs), self.pipeline_name)\n",
    "\n",
    "    def crossJoin(self, *args, **kwargs):\n",
    "        return CapturingDataFrame(self._log_and_apply(\"crossJoin\", *args, **kwargs), self.pipeline_name)\n",
    "\n",
    "    def union(self, *args, **kwargs):\n",
    "        return CapturingDataFrame(self._log_and_apply(\"union\", *args, **kwargs), self.pipeline_name)\n",
    "\n",
    "    def unionByName(self, *args, **kwargs):\n",
    "        return CapturingDataFrame(self._log_and_apply(\"unionByName\", *args, **kwargs), self.pipeline_name)\n",
    "\n",
    "    def intersect(self, *args, **kwargs):\n",
    "        return CapturingDataFrame(self._log_and_apply(\"intersect\", *args, **kwargs), self.pipeline_name)\n",
    "\n",
    "    def exceptAll(self, *args, **kwargs):\n",
    "        return CapturingDataFrame(self._log_and_apply(\"exceptAll\", *args, **kwargs), self.pipeline_name)\n",
    "\n",
    "    def intersectAll(self, *args, **kwargs):\n",
    "        return CapturingDataFrame(self._log_and_apply(\"intersectAll\", *args, **kwargs), self.pipeline_name)\n",
    "\n",
    "    def repartitionByRange(self, *args, **kwargs):\n",
    "        return CapturingDataFrame(self._log_and_apply(\"repartitionByRange\", *args, **kwargs), self.pipeline_name)\n",
    "\n",
    "    # Proxy methods for other DataFrame operations can be added as needed\n",
    "    def __getattr__(self, attr):\n",
    "        \"\"\"For any other DataFrame methods that are not explicitly wrapped, \n",
    "        use the original DataFrame methods.\"\"\"\n",
    "        return getattr(self._df, attr)\n",
    "\n",
    "# Wrapper function to capture the initial DataFrame and pipeline name\n",
    "def wrap_dataframe(df, pipeline_name):\n",
    "    return CapturingDataFrame(df, pipeline_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "122d63a9-836a-44fe-b85a-2f271d44a28f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from datetime import datetime\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Global variables\n",
    "write_logs_dir = \"/mnt/datalake/write_logs/\"  # Default log directory\n",
    "logs_initialized = False  # Flag to track directory initialization\n",
    "\n",
    "# Function to set logs directory path dynamically\n",
    "def set_write_logs_dir(path):\n",
    "    global write_logs_dir, logs_initialized\n",
    "    write_logs_dir = path\n",
    "    logs_initialized = False  # Reset the flag when path changes\n",
    "    print(f\"Logs directory set to: {write_logs_dir}\")\n",
    "\n",
    "# Function to initialize log directory (only once)\n",
    "def initialize_log_directory():\n",
    "    global logs_initialized\n",
    "    if logs_initialized:\n",
    "        return  # Skip if already initialized\n",
    "\n",
    "    try:\n",
    "        # Check if the directory exists, create if not\n",
    "        if not any(dbutils.fs.ls(write_logs_dir)):\n",
    "            dbutils.fs.mkdirs(write_logs_dir)  # Create the directory if it doesn't exist\n",
    "            print(f\"Log directory {write_logs_dir} created.\")\n",
    "        else:\n",
    "            print(f\"Log directory {write_logs_dir} already exists.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while creating directory {write_logs_dir}: {e}\")\n",
    "\n",
    "    logs_initialized = True  # Set flag to prevent redundant execution\n",
    "\n",
    "# Function to log write actions\n",
    "def log_write_action(action_name, details):\n",
    "    initialize_log_directory()  # Ensure the log directory is initialized only once\n",
    "\n",
    "    metadata = {\n",
    "        'uuid': str(unique_id),\n",
    "        'action_name': action_name,\n",
    "        'details': str(details),\n",
    "        'timestamp': str(datetime.now())\n",
    "    }\n",
    "\n",
    "    # Log into a Delta table (or any other logging system)\n",
    "    lineage_df = spark.createDataFrame([metadata])\n",
    "    lineage_df.write.format(\"delta\").mode(\"append\").save(write_logs_dir)\n",
    "\n",
    "# Class to wrap DataFrame and capture write actions\n",
    "class CapturingWriteDataFrame:\n",
    "    def __init__(self, spark_df, pipeline_name):\n",
    "        self._df = spark_df\n",
    "        self.pipeline_name = pipeline_name  # Store the pipeline name\n",
    "        self._write_mode = None  # This will hold the write mode\n",
    "        self._write_path = None  # This will hold the write path\n",
    "\n",
    "    def _log_write_and_apply(self, action_name, *args, **kwargs):\n",
    "        \"\"\"Helper function to log write actions.\"\"\"\n",
    "        log_write_action(action_name, {\"args\": args, \"kwargs\": kwargs, \"path\": self._write_path})\n",
    "        # Perform the actual write operation\n",
    "        return getattr(self._df, action_name)(*args, **kwargs)\n",
    "\n",
    "    def save(self, *args, **kwargs):\n",
    "        \"\"\"Override the save method to log the write operation.\"\"\"\n",
    "        return self._log_write_and_apply(\"save\", *args, **kwargs)\n",
    "\n",
    "    def insertInto(self, *args, **kwargs):\n",
    "        \"\"\"Override insertInto method for tables to log the write operation.\"\"\"\n",
    "        return self._log_write_and_apply(\"insertInto\", *args, **kwargs)\n",
    "\n",
    "    # Handle 'mode' method separately to capture the write mode\n",
    "    def mode(self, mode):\n",
    "        \"\"\"Override mode method to capture the write mode.\"\"\"\n",
    "        self._write_mode = mode\n",
    "        return self  # Return the current instance to allow chaining\n",
    "\n",
    "    # Capture the path (location) where the data is being written\n",
    "    def path(self, path):\n",
    "        \"\"\"Method to set the write path and log it.\"\"\"\n",
    "        self._write_path = path\n",
    "        return self  # Return self to allow method chaining\n",
    "\n",
    "    # Capture path for the specific write functions (parquet, csv, etc.)\n",
    "    def _capture_path(self, *args, **kwargs):\n",
    "        \"\"\"Capture path from any arguments or kwargs passed.\"\"\"\n",
    "        if args:\n",
    "            self._write_path = args[0]  # Capture from args if it's the first argument\n",
    "        elif 'path' in kwargs:\n",
    "            self._write_path = kwargs['path']\n",
    "\n",
    "    # Intercept and capture paths for all specific write functions like parquet, csv, etc.\n",
    "    def parquet(self, *args, **kwargs):\n",
    "        \"\"\"Override parquet write method to capture the path.\"\"\"\n",
    "        self._capture_path(*args, **kwargs)\n",
    "        return self._log_write_and_apply(\"parquet\", *args, **kwargs)\n",
    "\n",
    "    def csv(self, *args, **kwargs):\n",
    "        \"\"\"Override csv write method to capture the path.\"\"\"\n",
    "        self._capture_path(*args, **kwargs)\n",
    "        return self._log_write_and_apply(\"csv\", *args, **kwargs)\n",
    "\n",
    "    def json(self, *args, **kwargs):\n",
    "        \"\"\"Override json write method to capture the path.\"\"\"\n",
    "        self._capture_path(*args, **kwargs)\n",
    "        return self._log_write_and_apply(\"json\", *args, **kwargs)\n",
    "\n",
    "    # New `line_write` method to accept and log path\n",
    "    def line_write(self, path=None, *args, **kwargs):\n",
    "        \"\"\"Custom write method to handle DataFrame write operations with mode.\"\"\"\n",
    "        # If path is provided explicitly, use it\n",
    "        if path:\n",
    "            self._write_path = path\n",
    "        \n",
    "        # Log the write action before actually writing the data\n",
    "        log_write_action(\"line_write\", {\n",
    "            \"mode\": self._write_mode, \n",
    "            \"df_schema\": str(self._df.schema),\n",
    "            \"path\": self._write_path\n",
    "        })\n",
    "        \n",
    "        writer = self._df.write\n",
    "        if self._write_mode:\n",
    "            writer = writer.mode(self._write_mode)\n",
    "\n",
    "        # Check if the path is provided or set previously\n",
    "        if self._write_path:\n",
    "            # Log the actual write operation with path\n",
    "            return writer.parquet(self._write_path, *args, **kwargs)  # Assuming parquet, but can be adjusted for other formats\n",
    "        else:\n",
    "            return writer.parquet(*args, **kwargs)  # If no path, still write without a path\n",
    "\n",
    "    # Proxy methods for other DataFrame operations (excluding transformations)\n",
    "    def __getattr__(self, attr):\n",
    "        \"\"\"For any other DataFrame methods that are not explicitly wrapped, \n",
    "        use the original DataFrame methods.\"\"\"\n",
    "        return getattr(self._df, attr)\n",
    "\n",
    "# Wrapper function to capture the initial DataFrame and pipeline name\n",
    "def wrap_write_dataframe(df, pipeline_name):\n",
    "    return CapturingWriteDataFrame(df, pipeline_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b3c7cc6c-91f4-4236-a3f8-1ed0c060b9dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Define log directory for combined source and destination logs\n",
    "log_dir = \"/mnt/datalake/sql_activity_logs/\"  # Log path for both source and destination\n",
    "\n",
    "# Initialize the Spark session\n",
    "spark = SparkSession.builder.appName(\"SQLLogger\").getOrCreate()\n",
    "\n",
    "def set_sql_logs_dir(path):\n",
    "    global log_dir\n",
    "    log_dir = path\n",
    "    print(f'Sql logs dir set to: {log_dir}')\n",
    "\n",
    "def initialize_log_directory():\n",
    "    \"\"\"\n",
    "    Initializes log directory for both source and destination tables if it doesn't exist.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if the directory exists by listing the parent directory\n",
    "        dbutils.fs.mkdirs(log_dir)  # This creates the directory if it doesn't exist\n",
    "        print(f\"Directory created or already exists: {log_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error while creating directory {log_dir}: {str(e)}\")\n",
    "\n",
    "def log_sql_activity(query, source_tables, destination_tables):\n",
    "    \"\"\"\n",
    "    Log SQL activity, including both source and destination tables, and the query that was executed.\n",
    "    \"\"\"\n",
    "    # Generate a unique UUID for the current query\n",
    "    uuid_for_query = str(uuid.uuid4())\n",
    "    timestamp = str(datetime.now())\n",
    "    \n",
    "    # Combine source and destination tables\n",
    "    log_metadata = [{\n",
    "        'uuid': uuid_for_query,\n",
    "        'query': query,\n",
    "        'source_tables': source_tables,\n",
    "        'destination_tables': destination_tables,\n",
    "        'timestamp': timestamp\n",
    "    }]\n",
    "    \n",
    "    # Define schema explicitly\n",
    "    schema = StructType([\n",
    "        StructField(\"uuid\", StringType(), True),\n",
    "        StructField(\"query\", StringType(), True),\n",
    "        StructField(\"source_tables\", StringType(), True),\n",
    "        StructField(\"destination_tables\", StringType(), True),\n",
    "        StructField(\"timestamp\", StringType(), True)\n",
    "    ])\n",
    "    \n",
    "    # Create DataFrame with explicit schema\n",
    "    log_df = spark.createDataFrame(log_metadata, schema)\n",
    "    \n",
    "    # Write log information to a Delta table (append mode)\n",
    "    log_df.write.format(\"delta\").mode(\"append\").save(log_dir)\n",
    "\n",
    "\n",
    "def execute(query):\n",
    "    initialize_log_directory()\n",
    "    return SQLExecutor(spark).execute_sql(query)\n",
    "\n",
    "class SQLExecutor:\n",
    "    def __init__(self, spark: SparkSession):\n",
    "        self.spark = spark\n",
    "\n",
    "    def execute_sql(self, query):\n",
    "        \"\"\"\n",
    "        Execute the SQL query, track source and destination tables, and log the activity.\n",
    "        \"\"\"\n",
    "        # Extract source and destination tables from the query\n",
    "        source_tables, destination_tables = self._extract_tables_from_query(query)\n",
    "        \n",
    "        # If no destination tables found (like in UPDATE queries), log as \"unknown\"\n",
    "        if not destination_tables:\n",
    "            destination_tables = [\"unknown\"]\n",
    "        \n",
    "        # Log the source and destination tables along with the query\n",
    "        log_sql_activity(query, source_tables, destination_tables)\n",
    "        \n",
    "        # Execute the SQL query and return the result\n",
    "        return self.spark.sql(query)\n",
    "\n",
    "    def _extract_tables_from_query(self, query):\n",
    "        \"\"\"\n",
    "        Extract source and destination tables from the SQL query using regular expressions.\n",
    "        \"\"\"\n",
    "        # Regex to find tables mentioned in 'FROM' or 'JOIN'\n",
    "        source_tables = re.findall(r'FROM\\s+([a-zA-Z0-9_]+)|JOIN\\s+([a-zA-Z0-9_]+)', query)\n",
    "        source_tables = [table for sublist in source_tables for table in sublist if table]\n",
    "\n",
    "        # Regex to find tables mentioned in 'INSERT INTO' or 'CREATE TABLE AS' or 'CREATE TABLE IF NOT EXISTS'\n",
    "        destination_tables = re.findall(r'(?:INSERT\\s+INTO|CREATE\\s+TABLE(?:\\s+IF\\s+NOT\\s+EXISTS)?)\\s+([a-zA-Z0-9_]+)|CREATE\\s+TABLE\\s+AS\\s+([a-zA-Z0-9_]+)', query)\n",
    "        destination_tables = [table for sublist in destination_tables for table in sublist if table]\n",
    "\n",
    "        # For UPDATE or other queries where destination is not specified, mark as 'unknown'\n",
    "        if 'UPDATE' in query:\n",
    "            destination_tables = []\n",
    "\n",
    "        return source_tables, destination_tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a8659878-e54f-4e19-b63d-5f85cbb7ff6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#set the path as required by user\n",
    "def set_log_path(source = \"/mnt/datalake/source_logs/\",\n",
    "                 trans = \"/mnt/datalake/trans_logs/\",\n",
    "                 dest = \"/mnt/datalake/write_logs/\",\n",
    "                 sql =  \"/mnt/datalake/sql_activity_logs/\"):\n",
    "    set_source_logs_dir(source)\n",
    "    set_trans_logs_dir(trans)\n",
    "    set_write_logs_dir(dest)\n",
    "    set_sql_logs_dir(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea13679d-d254-4053-9b54-78923b9fe89a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Read the object and return the wrapped dataframe\n",
    "class DefObject:\n",
    "    def __init__(self, otype, pipeline_name):\n",
    "        self.otype = otype\n",
    "        self.pipeline_name = pipeline_name\n",
    "        set_uuid()\n",
    "\n",
    "    def read_object(self, query = None, file_path = None, file_type = None, **options):\n",
    "        if self.otype.lower() == \"file\":\n",
    "            if file_type.lower() == \"csv\":\n",
    "                odf = spark._read.csv(file_path, **options)  # Use _read to access the custom reader\n",
    "                df = wrap_dataframe(odf, self.pipeline_name) # wrap dataframe\n",
    "                return df\n",
    "\n",
    "            elif file_type.lower() == \"parquet\":\n",
    "                odf = spark._read.parquet(file_path, **options)  # Use _read to access the custom reader\n",
    "                df = wrap_dataframe(odf, self.pipeline_name) # wrap dataframe\n",
    "                return df\n",
    "\n",
    "            elif file_type.lower() == \"avro\":\n",
    "                odf = spark._read.format(\"avro\").load(file_path, **options)  # Use _read to access the custom reader\n",
    "                df = wrap_dataframe(odf, self.pipeline_name) # wrap dataframe\n",
    "                return df\n",
    "\n",
    "        elif self.otype.lower() == \"table\":\n",
    "            execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "348e1e70-4d84-4cf1-b933-af488e663b14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class WriteObject:\n",
    "    def __init__(self, save_type, pipeline_name):\n",
    "        self.pipeline_name = pipeline_name\n",
    "        self.save_type = save_type\n",
    "\n",
    "    def write_object(self, df, file_path, mode):\n",
    "        if self.save_type.lower() == \"file\":\n",
    "            # Write to destination (this will automatically log the write action)\n",
    "            df_write = wrap_write_dataframe(df, self.pipeline_name)\n",
    "            df_write.mode(mode).line_write(file_path)\n",
    "\n",
    "            print(f\"File written successfully at path: {file_path} with {mode} mode\")\n",
    "            combine_files()\n",
    "\n",
    "        elif self.save_type.lower() == \"table\":\n",
    "            pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42d04a9a-2c6f-4193-9fbb-a4e2fcb236d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline First_Pipeline started at 1743498807.9519837\nLogs directory set to: /mnt/datalake/source_logs/\nLogs directory set to: /mnt/datalake/trans_logs/\nLogs directory set to: /mnt/datalake/write_logs/\nSql logs dir set to: /mnt/datalake/Pipeline1/sql/sql_logs/\ne253a561-602b-4bdc-b5f4-2b613096c789\nDirectory created or already exists: /mnt/datalake/Pipeline1/sql/sql_logs/\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1711996001619652>:37\u001B[0m\n",
       "\u001B[1;32m     34\u001B[0m     \u001B[38;5;66;03m# Log the end of pipeline execution\u001B[39;00m\n",
       "\u001B[1;32m     35\u001B[0m     on_execution_end(start_time, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFirst_Pipeline\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[0;32m---> 37\u001B[0m etl_sql_pipeline()\n",
       "\n",
       "File \u001B[0;32m<command-1711996001619652>:30\u001B[0m, in \u001B[0;36metl_sql_pipeline\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m     17\u001B[0m insert_data_sql \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\n",
       "\u001B[1;32m     18\u001B[0m \u001B[38;5;124m            INSERT INTO employee_table5 VALUES\u001B[39m\n",
       "\u001B[1;32m     19\u001B[0m \u001B[38;5;124m            (1, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mJohn Doe\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEngineering\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, 1000),\u001B[39m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m     23\u001B[0m \u001B[38;5;124m            (5, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCharlie White\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSales\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, 1100);\u001B[39m\n",
       "\u001B[1;32m     24\u001B[0m \u001B[38;5;124m            \u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n",
       "\u001B[1;32m     26\u001B[0m select_query \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\n",
       "\u001B[1;32m     27\u001B[0m \u001B[38;5;124m                SELECT * FROM employee_table5;\u001B[39m\n",
       "\u001B[1;32m     28\u001B[0m \u001B[38;5;124m               \u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n",
       "\u001B[0;32m---> 30\u001B[0m \u001B[43mDefObject\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mFirst Pipeline\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_object\u001B[49m\u001B[43m(\u001B[49m\u001B[43msql_query\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     31\u001B[0m DefObject(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtable\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFirst Pipeline\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mread_object(insert_data_sql)\n",
       "\u001B[1;32m     32\u001B[0m DefObject(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtable\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFirst Pipeline\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mread_object(select_query)\n",
       "\n",
       "File \u001B[0;32m<command-1711996001619650>:26\u001B[0m, in \u001B[0;36mDefObject.read_object\u001B[0;34m(self, query, file_path, file_type, **options)\u001B[0m\n",
       "\u001B[1;32m     23\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39motype\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtable\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[0;32m---> 26\u001B[0m     \u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m<command-1711996001619648>:64\u001B[0m, in \u001B[0;36mexecute\u001B[0;34m(query)\u001B[0m\n",
       "\u001B[1;32m     62\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mexecute\u001B[39m(query):\n",
       "\u001B[1;32m     63\u001B[0m     initialize_log_directory()\n",
       "\u001B[0;32m---> 64\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSQLExecutor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mspark\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m<command-1711996001619648>:85\u001B[0m, in \u001B[0;36mSQLExecutor.execute_sql\u001B[0;34m(self, query)\u001B[0m\n",
       "\u001B[1;32m     82\u001B[0m log_sql_activity(query, source_tables, destination_tables)\n",
       "\u001B[1;32m     84\u001B[0m \u001B[38;5;66;03m# Execute the SQL query and return the result\u001B[39;00m\n",
       "\u001B[0;32m---> 85\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n",
       "\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Cannot create table ('`spark_catalog`.`default`.`employee_table5`'). The associated location ('dbfs:/user/hive/warehouse/employee_table5') is not empty and also not a Delta table."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-1711996001619652>:37\u001B[0m\n\u001B[1;32m     34\u001B[0m     \u001B[38;5;66;03m# Log the end of pipeline execution\u001B[39;00m\n\u001B[1;32m     35\u001B[0m     on_execution_end(start_time, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFirst_Pipeline\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 37\u001B[0m etl_sql_pipeline()\n\nFile \u001B[0;32m<command-1711996001619652>:30\u001B[0m, in \u001B[0;36metl_sql_pipeline\u001B[0;34m()\u001B[0m\n\u001B[1;32m     17\u001B[0m insert_data_sql \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;124m            INSERT INTO employee_table5 VALUES\u001B[39m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;124m            (1, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mJohn Doe\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEngineering\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, 1000),\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;124m            (5, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mCharlie White\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mSales\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, 1100);\u001B[39m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;124m            \u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m     26\u001B[0m select_query \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;124m                SELECT * FROM employee_table5;\u001B[39m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;124m               \u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[0;32m---> 30\u001B[0m \u001B[43mDefObject\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtable\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mFirst Pipeline\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_object\u001B[49m\u001B[43m(\u001B[49m\u001B[43msql_query\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     31\u001B[0m DefObject(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtable\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFirst Pipeline\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mread_object(insert_data_sql)\n\u001B[1;32m     32\u001B[0m DefObject(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtable\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFirst Pipeline\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mread_object(select_query)\n\nFile \u001B[0;32m<command-1711996001619650>:26\u001B[0m, in \u001B[0;36mDefObject.read_object\u001B[0;34m(self, query, file_path, file_type, **options)\u001B[0m\n\u001B[1;32m     23\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m df\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39motype\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtable\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m---> 26\u001B[0m     \u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m<command-1711996001619648>:64\u001B[0m, in \u001B[0;36mexecute\u001B[0;34m(query)\u001B[0m\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mexecute\u001B[39m(query):\n\u001B[1;32m     63\u001B[0m     initialize_log_directory()\n\u001B[0;32m---> 64\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mSQLExecutor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mspark\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m<command-1711996001619648>:85\u001B[0m, in \u001B[0;36mSQLExecutor.execute_sql\u001B[0;34m(self, query)\u001B[0m\n\u001B[1;32m     82\u001B[0m log_sql_activity(query, source_tables, destination_tables)\n\u001B[1;32m     84\u001B[0m \u001B[38;5;66;03m# Execute the SQL query and return the result\u001B[39;00m\n\u001B[0;32m---> 85\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Cannot create table ('`spark_catalog`.`default`.`employee_table5`'). The associated location ('dbfs:/user/hive/warehouse/employee_table5') is not empty and also not a Delta table.",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Cannot create table ('`spark_catalog`.`default`.`employee_table5`'). The associated location ('dbfs:/user/hive/warehouse/employee_table5') is not empty and also not a Delta table.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def etl_sql_pipeline():\n",
    "    # Log the start of the pipeline execution\n",
    "    start_time = on_execution_start(\"First_Pipeline\")\n",
    "\n",
    "    set_log_path(sql = \"/mnt/datalake/Pipeline1/sql/sql_logs/\")\n",
    "\n",
    "    sql_query =  \"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS employee_table5 (\n",
    "                    id INT,\n",
    "                    name STRING,\n",
    "                    department STRING,\n",
    "                    salary INT\n",
    "                )\n",
    "                USING DELTA;\n",
    "                \"\"\"\n",
    "    \n",
    "    insert_data_sql = \"\"\"\n",
    "                INSERT INTO employee_table5 VALUES\n",
    "                (1, 'John Doe', 'Engineering', 1000),\n",
    "                (2, 'Jane Smith', 'Marketing', 1200),\n",
    "                (3, 'Alice Johnson', 'Finance', 1400),\n",
    "                (4, 'Bob Brown', 'Engineering', 1300),\n",
    "                (5, 'Charlie White', 'Sales', 1100);\n",
    "                \"\"\"\n",
    "\n",
    "    select_query = \"\"\"\n",
    "                    SELECT * FROM employee_table5;\n",
    "                   \"\"\"\n",
    "\n",
    "    DefObject(\"table\", \"First Pipeline\").read_object(sql_query)\n",
    "    DefObject(\"table\", \"First Pipeline\").read_object(insert_data_sql)\n",
    "    DefObject(\"table\", \"First Pipeline\").read_object(select_query)\n",
    "\n",
    "    # Log the end of pipeline execution\n",
    "    on_execution_end(start_time, \"First_Pipeline\")\n",
    "\n",
    "etl_sql_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b000f86a-3c8b-48ad-925b-927fd1a4d899",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline First_Pipeline started at 1743498829.173399\nLogs directory set to: /mnt/datalake/Pipeline1/source1/source_logs/\nLogs directory set to: /mnt/datalake/Pipeline1/transformation1/trans_logs/\nLogs directory set to: /mnt/datalake/Pipeline1/destination1/write_logs/\nSql logs dir set to: /mnt/datalake/sql_activity_logs/\n82d9db3c-acea-4b91-a14a-da8c33e1930d\nLog directory /mnt/datalake/Pipeline1/source1/source_logs/ is ready.\nSource log successfully written for dbfs:/FileStore/files/source_data.csv to /mnt/datalake/Pipeline1/source1/source_logs/\nDirectory created or already exists: /mnt/datalake/sql_activity_logs/\nFile written successfully at path: /mnt/datalake/output/answer.parquet with overwrite mode\nPipeline First_Pipeline completed in 80.50483441352844 seconds\n"
     ]
    }
   ],
   "source": [
    "def run_etl_pipeline():\n",
    "    # Log the start of the pipeline execution\n",
    "    start_time = on_execution_start(\"First_Pipeline\")\n",
    "\n",
    "    set_log_path(source = \"/mnt/datalake/Pipeline1/source1/source_logs/\", \n",
    "                 trans = \"/mnt/datalake/Pipeline1/transformation1/trans_logs/\", \n",
    "                 dest = \"/mnt/datalake/Pipeline1/destination1/write_logs/\") #if they do not call this function default log files will be used\n",
    "\n",
    "    df = DefObject(\"file\", \"First_Pipeline\").read_object(file_path = 'dbfs:/FileStore/files/source_data.csv', \n",
    "                                                          file_type = \"csv\", header = True)\n",
    "\n",
    "    # Apply transformations\n",
    "    df = df.filter(\"amount > 100\")  # Filter step\n",
    "    df = df.groupBy(\"region\").agg(F.sum(\"amount\").alias(\"total_Profit\"))  # Group and aggregate\n",
    "    df = df.withColumnRenamed(\"total_profit\", \"sum_profit\")  # Rename column\n",
    "\n",
    "    # Additional transformations\n",
    "    df = df.orderBy(\"sum_profit\", ascending=False)  # Order by sum_amount\n",
    "    df = df.distinct()  # Remove duplicate rows\n",
    "    df = df.fillna({\"sum_profit\": 0})  # Fill missing values in the sum_amount column\n",
    "    df = df.withColumn(\"new_column\", F.lit(\"static_value\"))  # Add a new column with a static value\n",
    "\n",
    "    # Join with another DataFrame (create a sample DataFrame for the join)\n",
    "    df_other = spark.createDataFrame([(\"North\", 50), (\"South\", 30)], [\"region\", \"new_data\"])\n",
    "\n",
    "    # Perform the join before dropping the region column\n",
    "    df = df.join(df_other, on=\"region\", how=\"left\")  # Perform a left join on the region column\n",
    "\n",
    "    # Drop the region column after the join\n",
    "    df = df.drop(\"region\")  # Drop the region column\n",
    "\n",
    "    WriteObject(\"file\", \"First_Pipeline\").write_object(df, file_path = \"/mnt/datalake/output/answer.parquet\", mode = \"overwrite\")\n",
    "    \n",
    "    # Log the end of pipeline execution\n",
    "    on_execution_end(start_time, \"First_Pipeline\")\n",
    "\n",
    "# Run the pipeline\n",
    "run_etl_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d9cd3f1-3fde-4375-ba26-1ab4d989fb29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline First_Pipeline started at 1743062814.3082209\nLogs directory set to: /mnt/datalake/Pipeline1/source1/source_logs/\nLogs directory set to: /mnt/datalake/Pipeline1/transformation1/trans_logs/\nLogs directory set to: /mnt/datalake/Pipeline1/destination1/write_logs/\nSql logs dir set to: /mnt/datalake/sql_activity_logs/\n2706e19d-44f7-414c-aecc-6ff1a614351d\nLog directory /mnt/datalake/Pipeline1/source1/source_logs/ is ready.\nSource log successfully written for dbfs:/FileStore/files/mtcars.parquet to /mnt/datalake/Pipeline1/source1/source_logs/\nDirectory created or already exists: /mnt/datalake/sql_activity_logs/\nFile written successfully at path: /mnt/datalake/output/Parquetanswer.parquet with overwrite mode\n0679e0e6-b0b7-45d5-82e3-0d22f8d9686f\nLog directory /mnt/datalake/Pipeline1/source1/source_logs/ is ready.\nSource log successfully written for dbfs:/tmp/sample_data.avro to /mnt/datalake/Pipeline1/source1/source_logs/\nDirectory created or already exists: /mnt/datalake/sql_activity_logs/\nFile written successfully at path: /mnt/datalake/output/Avroanswer.parquet with overwrite mode\nPipeline First_Pipeline completed in 87.11229705810547 seconds\n"
     ]
    }
   ],
   "source": [
    "def run_etl_pipeline2():\n",
    "    # Log the start of the pipeline execution\n",
    "    start_time = on_execution_start(\"First_Pipeline\")\n",
    "\n",
    "    set_log_path(source = \"/mnt/datalake/Pipeline1/source1/source_logs/\", \n",
    "                 trans = \"/mnt/datalake/Pipeline1/transformation1/trans_logs/\", \n",
    "                 dest = \"/mnt/datalake/Pipeline1/destination1/write_logs/\") #if they do not call this function default log files will be used\n",
    "\n",
    "    df = DefObject(\"file\", \"First_Pipeline\").read_object(file_path = 'dbfs:/FileStore/files/mtcars.parquet', \n",
    "                                                          file_type = \"parquet\")\n",
    "\n",
    "    # Apply transformations\n",
    "    df = df.filter(\"cyl > 5\")  # Filter step\n",
    "\n",
    "    df = df.distinct()  # Remove duplicate rows\n",
    "\n",
    "    # Drop the region column after the join\n",
    "    df = df.drop(\"vs\")  # Drop the region column\n",
    "\n",
    "    WriteObject(\"file\", \"First_Pipeline\").write_object(df, file_path = \"/mnt/datalake/output/Parquetanswer.parquet\", mode = \"overwrite\")\n",
    "\n",
    "    df2 = DefObject(\"file\", \"First_Pipeline\").read_object(file_path = 'dbfs:/tmp/sample_data.avro', \n",
    "                                                        file_type = \"avro\", header = True)\n",
    "\n",
    "    # Apply transformations\n",
    "    df2 = df2.filter(\"Age> 5\")  # Filter step\n",
    "\n",
    "    df2 = df2.distinct()  # Remove duplicate rows\n",
    "\n",
    "    # Drop the region column after the join\n",
    "    df2 = df2.drop(\"JoiningDate\")  # Drop the region column\n",
    "\n",
    "    # Write to destination (e.g., parquet file)\n",
    "    # df.write.mode(\"overwrite\").parquet(\"/mnt/datalake/output.parquet\")\n",
    "\n",
    "    WriteObject(\"file\", \"First_Pipeline\").write_object(df2, file_path = \"/mnt/datalake/output/Avroanswer.parquet\", mode = \"overwrite\")\n",
    "\n",
    "    \n",
    "    # Log the end of pipeline execution\n",
    "    on_execution_end(start_time, \"First_Pipeline\")\n",
    "\n",
    "# Run the pipeline\n",
    "run_etl_pipeline2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2987d326-ef82-4b26-a50b-8f9b1bb5a5c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>additional_info</th><th>file_format</th><th>source_file</th><th>timestamp</th><th>uuid</th></tr></thead><tbody><tr><td>{'header': True}</td><td>avro</td><td>dbfs:/tmp/sample_data.avro</td><td>2025-03-27 08:07:38.735051</td><td>0679e0e6-b0b7-45d5-82e3-0d22f8d9686f</td></tr><tr><td></td><td>parquet</td><td>dbfs:/FileStore/files/mtcars.parquet</td><td>2025-03-27 08:06:54.407705</td><td>2706e19d-44f7-414c-aecc-6ff1a614351d</td></tr><tr><td>{'header': True}</td><td>csv</td><td>dbfs:/FileStore/files/source_data.csv</td><td>2025-03-27 08:05:19.426137</td><td>f4694785-cdcd-4087-b193-0ba735363acb</td></tr><tr><td>{'header': True}</td><td>csv</td><td>dbfs:/FileStore/files/source_data.csv</td><td>2025-03-26 10:06:30.070534</td><td>e97c3ac8-38be-44a2-aa98-9a545a6935ac</td></tr><tr><td>{'header': True}</td><td>csv</td><td>dbfs:/FileStore/files/source_data.csv</td><td>2025-03-26 10:05:52.917329</td><td>a0306fc0-74c7-4274-8ce8-dd19f09359ad</td></tr><tr><td>{'header': True}</td><td>avro</td><td>dbfs:/tmp/sample_data.avro</td><td>2025-03-26 09:22:09.736725</td><td>20bdfccc-fa44-4218-a391-1019371af7d9</td></tr><tr><td></td><td>parquet</td><td>dbfs:/FileStore/files/mtcars.parquet</td><td>2025-03-26 09:21:42.563067</td><td>596b60d8-ffe2-410d-a830-176df59d10c7</td></tr><tr><td>{'header': True}</td><td>csv</td><td>dbfs:/FileStore/files/source_data.csv</td><td>2025-03-26 09:20:54.757777</td><td>8cebedad-0008-4f4c-9ea3-b0c936237063</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "{'header': True}",
         "avro",
         "dbfs:/tmp/sample_data.avro",
         "2025-03-27 08:07:38.735051",
         "0679e0e6-b0b7-45d5-82e3-0d22f8d9686f"
        ],
        [
         "",
         "parquet",
         "dbfs:/FileStore/files/mtcars.parquet",
         "2025-03-27 08:06:54.407705",
         "2706e19d-44f7-414c-aecc-6ff1a614351d"
        ],
        [
         "{'header': True}",
         "csv",
         "dbfs:/FileStore/files/source_data.csv",
         "2025-03-27 08:05:19.426137",
         "f4694785-cdcd-4087-b193-0ba735363acb"
        ],
        [
         "{'header': True}",
         "csv",
         "dbfs:/FileStore/files/source_data.csv",
         "2025-03-26 10:06:30.070534",
         "e97c3ac8-38be-44a2-aa98-9a545a6935ac"
        ],
        [
         "{'header': True}",
         "csv",
         "dbfs:/FileStore/files/source_data.csv",
         "2025-03-26 10:05:52.917329",
         "a0306fc0-74c7-4274-8ce8-dd19f09359ad"
        ],
        [
         "{'header': True}",
         "avro",
         "dbfs:/tmp/sample_data.avro",
         "2025-03-26 09:22:09.736725",
         "20bdfccc-fa44-4218-a391-1019371af7d9"
        ],
        [
         "",
         "parquet",
         "dbfs:/FileStore/files/mtcars.parquet",
         "2025-03-26 09:21:42.563067",
         "596b60d8-ffe2-410d-a830-176df59d10c7"
        ],
        [
         "{'header': True}",
         "csv",
         "dbfs:/FileStore/files/source_data.csv",
         "2025-03-26 09:20:54.757777",
         "8cebedad-0008-4f4c-9ea3-b0c936237063"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "additional_info",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "file_format",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "source_file",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "uuid",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Source\n",
    "\n",
    "# Read lineage logs as a Delta table\n",
    "lineage_df = spark.read.format(\"delta\").load(\"/mnt/datalake/Pipeline1/source1/source_logs/\")\n",
    "\n",
    "# Show all rows without truncation\n",
    "lineage_df.sort(F.col(\"timestamp\").desc()).display(truncate=False, n=lineage_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c07a6b6c-4906-4b98-800b-29ca9602efe5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>details</th><th>timestamp</th><th>transformation_name</th><th>uuid</th></tr></thead><tbody><tr><td>{'args': ('JoiningDate',), 'kwargs': {}}</td><td>2025-03-27 08:07:52.069528</td><td>drop</td><td>0679e0e6-b0b7-45d5-82e3-0d22f8d9686f</td></tr><tr><td>{'args': (), 'kwargs': {}}</td><td>2025-03-27 08:07:48.294226</td><td>distinct</td><td>0679e0e6-b0b7-45d5-82e3-0d22f8d9686f</td></tr><tr><td>{'args': ('Age> 5',), 'kwargs': {}}</td><td>2025-03-27 08:07:43.698757</td><td>filter</td><td>0679e0e6-b0b7-45d5-82e3-0d22f8d9686f</td></tr><tr><td>{'args': ('vs',), 'kwargs': {}}</td><td>2025-03-27 08:07:08.096207</td><td>drop</td><td>2706e19d-44f7-414c-aecc-6ff1a614351d</td></tr><tr><td>{'args': (), 'kwargs': {}}</td><td>2025-03-27 08:07:03.627471</td><td>distinct</td><td>2706e19d-44f7-414c-aecc-6ff1a614351d</td></tr><tr><td>{'args': ('cyl > 5',), 'kwargs': {}}</td><td>2025-03-27 08:06:59.482955</td><td>filter</td><td>2706e19d-44f7-414c-aecc-6ff1a614351d</td></tr><tr><td>{'args': ('region',), 'kwargs': {}}</td><td>2025-03-27 08:06:18.256485</td><td>drop</td><td>f4694785-cdcd-4087-b193-0ba735363acb</td></tr><tr><td>{'args': (DataFrame[region: string, new_data: bigint],), 'kwargs': {'on': 'region', 'how': 'left'}}</td><td>2025-03-27 08:06:14.416994</td><td>join</td><td>f4694785-cdcd-4087-b193-0ba735363acb</td></tr><tr><td>{'args': ('new_column', Column<'static_value'>), 'kwargs': {}}</td><td>2025-03-27 08:06:09.872729</td><td>withColumn</td><td>f4694785-cdcd-4087-b193-0ba735363acb</td></tr><tr><td>{'args': ({'sum_profit': 0},), 'kwargs': {}}</td><td>2025-03-27 08:06:03.098294</td><td>fillna</td><td>f4694785-cdcd-4087-b193-0ba735363acb</td></tr><tr><td>{'args': (), 'kwargs': {}}</td><td>2025-03-27 08:05:57.024610</td><td>distinct</td><td>f4694785-cdcd-4087-b193-0ba735363acb</td></tr><tr><td>{'args': ('sum_profit',), 'kwargs': {'ascending': False}}</td><td>2025-03-27 08:05:51.607075</td><td>orderBy</td><td>f4694785-cdcd-4087-b193-0ba735363acb</td></tr><tr><td>{'args': ('total_profit', 'sum_profit'), 'kwargs': {}}</td><td>2025-03-27 08:05:47.810618</td><td>withColumnRenamed</td><td>f4694785-cdcd-4087-b193-0ba735363acb</td></tr><tr><td>{'args': (Column<'sum(amount) AS total_Profit'>,), 'kwargs': {}}</td><td>2025-03-27 08:05:43.319715</td><td>agg</td><td>f4694785-cdcd-4087-b193-0ba735363acb</td></tr><tr><td>{'args': ('region',), 'kwargs': {}}</td><td>2025-03-27 08:05:37.840896</td><td>groupBy</td><td>f4694785-cdcd-4087-b193-0ba735363acb</td></tr><tr><td>{'args': ('amount > 100',), 'kwargs': {}}</td><td>2025-03-27 08:05:28.890664</td><td>filter</td><td>f4694785-cdcd-4087-b193-0ba735363acb</td></tr><tr><td>{'args': ('region',), 'kwargs': {}}</td><td>2025-03-26 10:07:01.474868</td><td>drop</td><td>e97c3ac8-38be-44a2-aa98-9a545a6935ac</td></tr><tr><td>{'args': (DataFrame[region: string, new_data: bigint],), 'kwargs': {'on': 'region', 'how': 'left'}}</td><td>2025-03-26 10:06:58.503444</td><td>join</td><td>e97c3ac8-38be-44a2-aa98-9a545a6935ac</td></tr><tr><td>{'args': ('new_column', Column<'static_value'>), 'kwargs': {}}</td><td>2025-03-26 10:06:56.157425</td><td>withColumn</td><td>e97c3ac8-38be-44a2-aa98-9a545a6935ac</td></tr><tr><td>{'args': ({'sum_profit': 0},), 'kwargs': {}}</td><td>2025-03-26 10:06:51.847327</td><td>fillna</td><td>e97c3ac8-38be-44a2-aa98-9a545a6935ac</td></tr><tr><td>{'args': (), 'kwargs': {}}</td><td>2025-03-26 10:06:48.469336</td><td>distinct</td><td>e97c3ac8-38be-44a2-aa98-9a545a6935ac</td></tr><tr><td>{'args': ('sum_profit',), 'kwargs': {'ascending': False}}</td><td>2025-03-26 10:06:45.149999</td><td>orderBy</td><td>e97c3ac8-38be-44a2-aa98-9a545a6935ac</td></tr><tr><td>{'args': ('total_profit', 'sum_profit'), 'kwargs': {}}</td><td>2025-03-26 10:06:42.461104</td><td>withColumnRenamed</td><td>e97c3ac8-38be-44a2-aa98-9a545a6935ac</td></tr><tr><td>{'args': (Column<'sum(amount) AS total_Profit'>,), 'kwargs': {}}</td><td>2025-03-26 10:06:39.724818</td><td>agg</td><td>e97c3ac8-38be-44a2-aa98-9a545a6935ac</td></tr><tr><td>{'args': ('region',), 'kwargs': {}}</td><td>2025-03-26 10:06:36.555283</td><td>groupBy</td><td>e97c3ac8-38be-44a2-aa98-9a545a6935ac</td></tr><tr><td>{'args': ('amount > 100',), 'kwargs': {}}</td><td>2025-03-26 10:06:34.038897</td><td>filter</td><td>e97c3ac8-38be-44a2-aa98-9a545a6935ac</td></tr><tr><td>{'args': ('JoiningDate',), 'kwargs': {}}</td><td>2025-03-26 09:22:19.878768</td><td>drop</td><td>20bdfccc-fa44-4218-a391-1019371af7d9</td></tr><tr><td>{'args': (), 'kwargs': {}}</td><td>2025-03-26 09:22:17.225162</td><td>distinct</td><td>20bdfccc-fa44-4218-a391-1019371af7d9</td></tr><tr><td>{'args': ('Age> 5',), 'kwargs': {}}</td><td>2025-03-26 09:22:13.737748</td><td>filter</td><td>20bdfccc-fa44-4218-a391-1019371af7d9</td></tr><tr><td>{'args': ('vs',), 'kwargs': {}}</td><td>2025-03-26 09:21:52.905821</td><td>drop</td><td>596b60d8-ffe2-410d-a830-176df59d10c7</td></tr><tr><td>{'args': (), 'kwargs': {}}</td><td>2025-03-26 09:21:48.173015</td><td>distinct</td><td>596b60d8-ffe2-410d-a830-176df59d10c7</td></tr><tr><td>{'args': ('cyl > 5',), 'kwargs': {}}</td><td>2025-03-26 09:21:45.749649</td><td>filter</td><td>596b60d8-ffe2-410d-a830-176df59d10c7</td></tr><tr><td>{'args': ('region',), 'kwargs': {}}</td><td>2025-03-26 09:21:26.050580</td><td>drop</td><td>8cebedad-0008-4f4c-9ea3-b0c936237063</td></tr><tr><td>{'args': (DataFrame[region: string, new_data: bigint],), 'kwargs': {'on': 'region', 'how': 'left'}}</td><td>2025-03-26 09:21:23.386503</td><td>join</td><td>8cebedad-0008-4f4c-9ea3-b0c936237063</td></tr><tr><td>{'args': ('new_column', Column<'static_value'>), 'kwargs': {}}</td><td>2025-03-26 09:21:20.399158</td><td>withColumn</td><td>8cebedad-0008-4f4c-9ea3-b0c936237063</td></tr><tr><td>{'args': ({'sum_profit': 0},), 'kwargs': {}}</td><td>2025-03-26 09:21:18.010204</td><td>fillna</td><td>8cebedad-0008-4f4c-9ea3-b0c936237063</td></tr><tr><td>{'args': (), 'kwargs': {}}</td><td>2025-03-26 09:21:15.117778</td><td>distinct</td><td>8cebedad-0008-4f4c-9ea3-b0c936237063</td></tr><tr><td>{'args': ('sum_profit',), 'kwargs': {'ascending': False}}</td><td>2025-03-26 09:21:12.320164</td><td>orderBy</td><td>8cebedad-0008-4f4c-9ea3-b0c936237063</td></tr><tr><td>{'args': ('total_profit', 'sum_profit'), 'kwargs': {}}</td><td>2025-03-26 09:21:10.140099</td><td>withColumnRenamed</td><td>8cebedad-0008-4f4c-9ea3-b0c936237063</td></tr><tr><td>{'args': (Column<'sum(amount) AS total_Profit'>,), 'kwargs': {}}</td><td>2025-03-26 09:21:05.321153</td><td>agg</td><td>8cebedad-0008-4f4c-9ea3-b0c936237063</td></tr><tr><td>{'args': ('region',), 'kwargs': {}}</td><td>2025-03-26 09:21:01.710217</td><td>groupBy</td><td>8cebedad-0008-4f4c-9ea3-b0c936237063</td></tr><tr><td>{'args': ('amount > 100',), 'kwargs': {}}</td><td>2025-03-26 09:20:58.183347</td><td>filter</td><td>8cebedad-0008-4f4c-9ea3-b0c936237063</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "{'args': ('JoiningDate',), 'kwargs': {}}",
         "2025-03-27 08:07:52.069528",
         "drop",
         "0679e0e6-b0b7-45d5-82e3-0d22f8d9686f"
        ],
        [
         "{'args': (), 'kwargs': {}}",
         "2025-03-27 08:07:48.294226",
         "distinct",
         "0679e0e6-b0b7-45d5-82e3-0d22f8d9686f"
        ],
        [
         "{'args': ('Age> 5',), 'kwargs': {}}",
         "2025-03-27 08:07:43.698757",
         "filter",
         "0679e0e6-b0b7-45d5-82e3-0d22f8d9686f"
        ],
        [
         "{'args': ('vs',), 'kwargs': {}}",
         "2025-03-27 08:07:08.096207",
         "drop",
         "2706e19d-44f7-414c-aecc-6ff1a614351d"
        ],
        [
         "{'args': (), 'kwargs': {}}",
         "2025-03-27 08:07:03.627471",
         "distinct",
         "2706e19d-44f7-414c-aecc-6ff1a614351d"
        ],
        [
         "{'args': ('cyl > 5',), 'kwargs': {}}",
         "2025-03-27 08:06:59.482955",
         "filter",
         "2706e19d-44f7-414c-aecc-6ff1a614351d"
        ],
        [
         "{'args': ('region',), 'kwargs': {}}",
         "2025-03-27 08:06:18.256485",
         "drop",
         "f4694785-cdcd-4087-b193-0ba735363acb"
        ],
        [
         "{'args': (DataFrame[region: string, new_data: bigint],), 'kwargs': {'on': 'region', 'how': 'left'}}",
         "2025-03-27 08:06:14.416994",
         "join",
         "f4694785-cdcd-4087-b193-0ba735363acb"
        ],
        [
         "{'args': ('new_column', Column<'static_value'>), 'kwargs': {}}",
         "2025-03-27 08:06:09.872729",
         "withColumn",
         "f4694785-cdcd-4087-b193-0ba735363acb"
        ],
        [
         "{'args': ({'sum_profit': 0},), 'kwargs': {}}",
         "2025-03-27 08:06:03.098294",
         "fillna",
         "f4694785-cdcd-4087-b193-0ba735363acb"
        ],
        [
         "{'args': (), 'kwargs': {}}",
         "2025-03-27 08:05:57.024610",
         "distinct",
         "f4694785-cdcd-4087-b193-0ba735363acb"
        ],
        [
         "{'args': ('sum_profit',), 'kwargs': {'ascending': False}}",
         "2025-03-27 08:05:51.607075",
         "orderBy",
         "f4694785-cdcd-4087-b193-0ba735363acb"
        ],
        [
         "{'args': ('total_profit', 'sum_profit'), 'kwargs': {}}",
         "2025-03-27 08:05:47.810618",
         "withColumnRenamed",
         "f4694785-cdcd-4087-b193-0ba735363acb"
        ],
        [
         "{'args': (Column<'sum(amount) AS total_Profit'>,), 'kwargs': {}}",
         "2025-03-27 08:05:43.319715",
         "agg",
         "f4694785-cdcd-4087-b193-0ba735363acb"
        ],
        [
         "{'args': ('region',), 'kwargs': {}}",
         "2025-03-27 08:05:37.840896",
         "groupBy",
         "f4694785-cdcd-4087-b193-0ba735363acb"
        ],
        [
         "{'args': ('amount > 100',), 'kwargs': {}}",
         "2025-03-27 08:05:28.890664",
         "filter",
         "f4694785-cdcd-4087-b193-0ba735363acb"
        ],
        [
         "{'args': ('region',), 'kwargs': {}}",
         "2025-03-26 10:07:01.474868",
         "drop",
         "e97c3ac8-38be-44a2-aa98-9a545a6935ac"
        ],
        [
         "{'args': (DataFrame[region: string, new_data: bigint],), 'kwargs': {'on': 'region', 'how': 'left'}}",
         "2025-03-26 10:06:58.503444",
         "join",
         "e97c3ac8-38be-44a2-aa98-9a545a6935ac"
        ],
        [
         "{'args': ('new_column', Column<'static_value'>), 'kwargs': {}}",
         "2025-03-26 10:06:56.157425",
         "withColumn",
         "e97c3ac8-38be-44a2-aa98-9a545a6935ac"
        ],
        [
         "{'args': ({'sum_profit': 0},), 'kwargs': {}}",
         "2025-03-26 10:06:51.847327",
         "fillna",
         "e97c3ac8-38be-44a2-aa98-9a545a6935ac"
        ],
        [
         "{'args': (), 'kwargs': {}}",
         "2025-03-26 10:06:48.469336",
         "distinct",
         "e97c3ac8-38be-44a2-aa98-9a545a6935ac"
        ],
        [
         "{'args': ('sum_profit',), 'kwargs': {'ascending': False}}",
         "2025-03-26 10:06:45.149999",
         "orderBy",
         "e97c3ac8-38be-44a2-aa98-9a545a6935ac"
        ],
        [
         "{'args': ('total_profit', 'sum_profit'), 'kwargs': {}}",
         "2025-03-26 10:06:42.461104",
         "withColumnRenamed",
         "e97c3ac8-38be-44a2-aa98-9a545a6935ac"
        ],
        [
         "{'args': (Column<'sum(amount) AS total_Profit'>,), 'kwargs': {}}",
         "2025-03-26 10:06:39.724818",
         "agg",
         "e97c3ac8-38be-44a2-aa98-9a545a6935ac"
        ],
        [
         "{'args': ('region',), 'kwargs': {}}",
         "2025-03-26 10:06:36.555283",
         "groupBy",
         "e97c3ac8-38be-44a2-aa98-9a545a6935ac"
        ],
        [
         "{'args': ('amount > 100',), 'kwargs': {}}",
         "2025-03-26 10:06:34.038897",
         "filter",
         "e97c3ac8-38be-44a2-aa98-9a545a6935ac"
        ],
        [
         "{'args': ('JoiningDate',), 'kwargs': {}}",
         "2025-03-26 09:22:19.878768",
         "drop",
         "20bdfccc-fa44-4218-a391-1019371af7d9"
        ],
        [
         "{'args': (), 'kwargs': {}}",
         "2025-03-26 09:22:17.225162",
         "distinct",
         "20bdfccc-fa44-4218-a391-1019371af7d9"
        ],
        [
         "{'args': ('Age> 5',), 'kwargs': {}}",
         "2025-03-26 09:22:13.737748",
         "filter",
         "20bdfccc-fa44-4218-a391-1019371af7d9"
        ],
        [
         "{'args': ('vs',), 'kwargs': {}}",
         "2025-03-26 09:21:52.905821",
         "drop",
         "596b60d8-ffe2-410d-a830-176df59d10c7"
        ],
        [
         "{'args': (), 'kwargs': {}}",
         "2025-03-26 09:21:48.173015",
         "distinct",
         "596b60d8-ffe2-410d-a830-176df59d10c7"
        ],
        [
         "{'args': ('cyl > 5',), 'kwargs': {}}",
         "2025-03-26 09:21:45.749649",
         "filter",
         "596b60d8-ffe2-410d-a830-176df59d10c7"
        ],
        [
         "{'args': ('region',), 'kwargs': {}}",
         "2025-03-26 09:21:26.050580",
         "drop",
         "8cebedad-0008-4f4c-9ea3-b0c936237063"
        ],
        [
         "{'args': (DataFrame[region: string, new_data: bigint],), 'kwargs': {'on': 'region', 'how': 'left'}}",
         "2025-03-26 09:21:23.386503",
         "join",
         "8cebedad-0008-4f4c-9ea3-b0c936237063"
        ],
        [
         "{'args': ('new_column', Column<'static_value'>), 'kwargs': {}}",
         "2025-03-26 09:21:20.399158",
         "withColumn",
         "8cebedad-0008-4f4c-9ea3-b0c936237063"
        ],
        [
         "{'args': ({'sum_profit': 0},), 'kwargs': {}}",
         "2025-03-26 09:21:18.010204",
         "fillna",
         "8cebedad-0008-4f4c-9ea3-b0c936237063"
        ],
        [
         "{'args': (), 'kwargs': {}}",
         "2025-03-26 09:21:15.117778",
         "distinct",
         "8cebedad-0008-4f4c-9ea3-b0c936237063"
        ],
        [
         "{'args': ('sum_profit',), 'kwargs': {'ascending': False}}",
         "2025-03-26 09:21:12.320164",
         "orderBy",
         "8cebedad-0008-4f4c-9ea3-b0c936237063"
        ],
        [
         "{'args': ('total_profit', 'sum_profit'), 'kwargs': {}}",
         "2025-03-26 09:21:10.140099",
         "withColumnRenamed",
         "8cebedad-0008-4f4c-9ea3-b0c936237063"
        ],
        [
         "{'args': (Column<'sum(amount) AS total_Profit'>,), 'kwargs': {}}",
         "2025-03-26 09:21:05.321153",
         "agg",
         "8cebedad-0008-4f4c-9ea3-b0c936237063"
        ],
        [
         "{'args': ('region',), 'kwargs': {}}",
         "2025-03-26 09:21:01.710217",
         "groupBy",
         "8cebedad-0008-4f4c-9ea3-b0c936237063"
        ],
        [
         "{'args': ('amount > 100',), 'kwargs': {}}",
         "2025-03-26 09:20:58.183347",
         "filter",
         "8cebedad-0008-4f4c-9ea3-b0c936237063"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "details",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "transformation_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "uuid",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Transformation\n",
    "\n",
    "# Read lineage logs as a Delta table\n",
    "lineage_df = spark.read.format(\"delta\").load(\"/mnt/datalake/Pipeline1/transformation1/trans_logs/\")\n",
    "\n",
    "# Show all rows without truncation\n",
    "lineage_df.sort(F.col(\"timestamp\").desc()).display(truncate=False, n=lineage_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "834adc2c-8be6-460c-a70a-e6b1b3f594f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>action_name</th><th>details</th><th>timestamp</th><th>uuid</th></tr></thead><tbody><tr><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('ID', IntegerType(), True), StructField('Name', StringType(), True), StructField('Age', IntegerType(), True), StructField('Country', StringType(), True), StructField('Salary', FloatType(), True)])\", 'path': '/mnt/datalake/output/Avroanswer.parquet'}</td><td>2025-03-27 08:07:57.835065</td><td>0679e0e6-b0b7-45d5-82e3-0d22f8d9686f</td></tr><tr><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('model', StringType(), True), StructField('mpg', DoubleType(), True), StructField('cyl', IntegerType(), True), StructField('disp', DoubleType(), True), StructField('hp', IntegerType(), True), StructField('drat', DoubleType(), True), StructField('wt', DoubleType(), True), StructField('qsec', DoubleType(), True), StructField('am', IntegerType(), True), StructField('gear', IntegerType(), True), StructField('carb', IntegerType(), True)])\", 'path': '/mnt/datalake/output/Parquetanswer.parquet'}</td><td>2025-03-27 08:07:13.247953</td><td>2706e19d-44f7-414c-aecc-6ff1a614351d</td></tr><tr><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-27 08:06:21.502141</td><td>f4694785-cdcd-4087-b193-0ba735363acb</td></tr><tr><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-26 10:07:04.346735</td><td>e97c3ac8-38be-44a2-aa98-9a545a6935ac</td></tr><tr><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('ID', IntegerType(), True), StructField('Name', StringType(), True), StructField('Age', IntegerType(), True), StructField('Country', StringType(), True), StructField('Salary', FloatType(), True)])\", 'path': '/mnt/datalake/output/Avroanswer.parquet'}</td><td>2025-03-26 09:22:22.331375</td><td>20bdfccc-fa44-4218-a391-1019371af7d9</td></tr><tr><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('model', StringType(), True), StructField('mpg', DoubleType(), True), StructField('cyl', IntegerType(), True), StructField('disp', DoubleType(), True), StructField('hp', IntegerType(), True), StructField('drat', DoubleType(), True), StructField('wt', DoubleType(), True), StructField('qsec', DoubleType(), True), StructField('am', IntegerType(), True), StructField('gear', IntegerType(), True), StructField('carb', IntegerType(), True)])\", 'path': '/mnt/datalake/output/Parquetanswer.parquet'}</td><td>2025-03-26 09:21:56.889266</td><td>596b60d8-ffe2-410d-a830-176df59d10c7</td></tr><tr><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-26 09:21:28.914155</td><td>8cebedad-0008-4f4c-9ea3-b0c936237063</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('ID', IntegerType(), True), StructField('Name', StringType(), True), StructField('Age', IntegerType(), True), StructField('Country', StringType(), True), StructField('Salary', FloatType(), True)])\", 'path': '/mnt/datalake/output/Avroanswer.parquet'}",
         "2025-03-27 08:07:57.835065",
         "0679e0e6-b0b7-45d5-82e3-0d22f8d9686f"
        ],
        [
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('model', StringType(), True), StructField('mpg', DoubleType(), True), StructField('cyl', IntegerType(), True), StructField('disp', DoubleType(), True), StructField('hp', IntegerType(), True), StructField('drat', DoubleType(), True), StructField('wt', DoubleType(), True), StructField('qsec', DoubleType(), True), StructField('am', IntegerType(), True), StructField('gear', IntegerType(), True), StructField('carb', IntegerType(), True)])\", 'path': '/mnt/datalake/output/Parquetanswer.parquet'}",
         "2025-03-27 08:07:13.247953",
         "2706e19d-44f7-414c-aecc-6ff1a614351d"
        ],
        [
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-27 08:06:21.502141",
         "f4694785-cdcd-4087-b193-0ba735363acb"
        ],
        [
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-26 10:07:04.346735",
         "e97c3ac8-38be-44a2-aa98-9a545a6935ac"
        ],
        [
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('ID', IntegerType(), True), StructField('Name', StringType(), True), StructField('Age', IntegerType(), True), StructField('Country', StringType(), True), StructField('Salary', FloatType(), True)])\", 'path': '/mnt/datalake/output/Avroanswer.parquet'}",
         "2025-03-26 09:22:22.331375",
         "20bdfccc-fa44-4218-a391-1019371af7d9"
        ],
        [
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('model', StringType(), True), StructField('mpg', DoubleType(), True), StructField('cyl', IntegerType(), True), StructField('disp', DoubleType(), True), StructField('hp', IntegerType(), True), StructField('drat', DoubleType(), True), StructField('wt', DoubleType(), True), StructField('qsec', DoubleType(), True), StructField('am', IntegerType(), True), StructField('gear', IntegerType(), True), StructField('carb', IntegerType(), True)])\", 'path': '/mnt/datalake/output/Parquetanswer.parquet'}",
         "2025-03-26 09:21:56.889266",
         "596b60d8-ffe2-410d-a830-176df59d10c7"
        ],
        [
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-26 09:21:28.914155",
         "8cebedad-0008-4f4c-9ea3-b0c936237063"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "action_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "details",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "uuid",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Destination\n",
    "\n",
    "# Read lineage logs as a Delta table\n",
    "lineage_df = spark.read.format(\"delta\").load(\"/mnt/datalake/Pipeline1/destination1/write_logs/\")\n",
    "\n",
    "# Show all rows without truncation\n",
    "lineage_df.sort(F.col(\"timestamp\").desc()).display(truncate=False, n=lineage_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49d48133-1257-443a-b8ea-17960b6d12f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>uuid</th><th>query</th><th>source_tables</th><th>destination_tables</th><th>timestamp</th></tr></thead><tbody><tr><td>c5ba42d2-0001-4ffc-8ce6-f6c5fe965202</td><td>\n",
       "                CREATE TABLE IF NOT EXISTS employee_table5 (\n",
       "                    id INT,\n",
       "                    name STRING,\n",
       "                    department STRING,\n",
       "                    salary INT\n",
       "                )\n",
       "                USING DELTA;\n",
       "                </td><td>[]</td><td>[employee_table5]</td><td>2025-04-01 09:13:28.039220</td></tr><tr><td>4596d267-d623-4fb6-bd7b-d9ebda2943b1</td><td>\n",
       "                    SELECT * FROM employee_table5;\n",
       "                   </td><td>[employee_table5]</td><td>[unknown]</td><td>2025-03-27 08:05:12.680028</td></tr><tr><td>6ea268cd-1fda-41e4-9897-772509bcb2cc</td><td>\n",
       "                INSERT INTO employee_table5 VALUES\n",
       "                (1, 'John Doe', 'Engineering', 1000),\n",
       "                (2, 'Jane Smith', 'Marketing', 1200),\n",
       "                (3, 'Alice Johnson', 'Finance', 1400),\n",
       "                (4, 'Bob Brown', 'Engineering', 1300),\n",
       "                (5, 'Charlie White', 'Sales', 1100);\n",
       "                </td><td>[]</td><td>[employee_table5]</td><td>2025-03-27 08:05:01.238966</td></tr><tr><td>35893c67-bf60-4bb9-bc5f-885b858a4f65</td><td>\n",
       "                CREATE TABLE IF NOT EXISTS employee_table5 (\n",
       "                    id INT,\n",
       "                    name STRING,\n",
       "                    department STRING,\n",
       "                    salary INT\n",
       "                )\n",
       "                USING DELTA;\n",
       "                </td><td>[]</td><td>[employee_table5]</td><td>2025-03-27 08:04:49.613385</td></tr><tr><td>1cba944a-0967-40d8-9bb8-1838dec3ffc5</td><td>\n",
       "                CREATE TABLE IF NOT EXISTS employee_table4 (\n",
       "                    id INT,\n",
       "                    name STRING,\n",
       "                    department STRING,\n",
       "                    salary INT\n",
       "                )\n",
       "                USING DELTA;\n",
       "                </td><td>[]</td><td>[employee_table4]</td><td>2025-03-27 08:03:19.809104</td></tr><tr><td>66659ad8-d13f-4799-abfd-37b76be491f7</td><td>\n",
       "                    SELECT * FROM employee_table4;\n",
       "                   </td><td>[employee_table4]</td><td>[unknown]</td><td>2025-03-26 10:09:36.290130</td></tr><tr><td>31549cd9-c816-4f9f-93da-929500f82fe2</td><td>\n",
       "                INSERT INTO employee_table4 VALUES\n",
       "                (1, 'John Doe', 'Engineering', 1000),\n",
       "                (2, 'Jane Smith', 'Marketing', 1200),\n",
       "                (3, 'Alice Johnson', 'Finance', 1400),\n",
       "                (4, 'Bob Brown', 'Engineering', 1300),\n",
       "                (5, 'Charlie White', 'Sales', 1100);\n",
       "                </td><td>[]</td><td>[employee_table4]</td><td>2025-03-26 10:09:31.829305</td></tr><tr><td>3ac6c0ce-9837-4308-980c-2e8bed25004b</td><td>\n",
       "                CREATE TABLE IF NOT EXISTS employee_table4 (\n",
       "                    id INT,\n",
       "                    name STRING,\n",
       "                    department STRING,\n",
       "                    salary INT\n",
       "                )\n",
       "                USING DELTA;\n",
       "                </td><td>[]</td><td>[employee_table4]</td><td>2025-03-26 10:09:26.634296</td></tr><tr><td>ee6b5b2a-af26-43a3-b38f-edbd1476c377</td><td>\n",
       "                    SELECT * FROM employee_table4;\n",
       "                   </td><td>[employee_table4]</td><td>[unknown]</td><td>2025-03-26 09:20:49.919760</td></tr><tr><td>9ace40ec-a6b0-4ff2-bddf-1ad9949098ce</td><td>\n",
       "                INSERT INTO employee_table4 VALUES\n",
       "                (1, 'John Doe', 'Engineering', 1000),\n",
       "                (2, 'Jane Smith', 'Marketing', 1200),\n",
       "                (3, 'Alice Johnson', 'Finance', 1400),\n",
       "                (4, 'Bob Brown', 'Engineering', 1300),\n",
       "                (5, 'Charlie White', 'Sales', 1100);\n",
       "                </td><td>[]</td><td>[employee_table4]</td><td>2025-03-26 09:20:44.278997</td></tr><tr><td>d1791be4-a5c2-4278-99e2-3c651a3ad85d</td><td>\n",
       "                CREATE TABLE IF NOT EXISTS employee_table4 (\n",
       "                    id INT,\n",
       "                    name STRING,\n",
       "                    department STRING,\n",
       "                    salary INT\n",
       "                )\n",
       "                USING DELTA;\n",
       "                </td><td>[]</td><td>[employee_table4]</td><td>2025-03-26 09:20:38.031664</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "c5ba42d2-0001-4ffc-8ce6-f6c5fe965202",
         "\n                CREATE TABLE IF NOT EXISTS employee_table5 (\n                    id INT,\n                    name STRING,\n                    department STRING,\n                    salary INT\n                )\n                USING DELTA;\n                ",
         "[]",
         "[employee_table5]",
         "2025-04-01 09:13:28.039220"
        ],
        [
         "4596d267-d623-4fb6-bd7b-d9ebda2943b1",
         "\n                    SELECT * FROM employee_table5;\n                   ",
         "[employee_table5]",
         "[unknown]",
         "2025-03-27 08:05:12.680028"
        ],
        [
         "6ea268cd-1fda-41e4-9897-772509bcb2cc",
         "\n                INSERT INTO employee_table5 VALUES\n                (1, 'John Doe', 'Engineering', 1000),\n                (2, 'Jane Smith', 'Marketing', 1200),\n                (3, 'Alice Johnson', 'Finance', 1400),\n                (4, 'Bob Brown', 'Engineering', 1300),\n                (5, 'Charlie White', 'Sales', 1100);\n                ",
         "[]",
         "[employee_table5]",
         "2025-03-27 08:05:01.238966"
        ],
        [
         "35893c67-bf60-4bb9-bc5f-885b858a4f65",
         "\n                CREATE TABLE IF NOT EXISTS employee_table5 (\n                    id INT,\n                    name STRING,\n                    department STRING,\n                    salary INT\n                )\n                USING DELTA;\n                ",
         "[]",
         "[employee_table5]",
         "2025-03-27 08:04:49.613385"
        ],
        [
         "1cba944a-0967-40d8-9bb8-1838dec3ffc5",
         "\n                CREATE TABLE IF NOT EXISTS employee_table4 (\n                    id INT,\n                    name STRING,\n                    department STRING,\n                    salary INT\n                )\n                USING DELTA;\n                ",
         "[]",
         "[employee_table4]",
         "2025-03-27 08:03:19.809104"
        ],
        [
         "66659ad8-d13f-4799-abfd-37b76be491f7",
         "\n                    SELECT * FROM employee_table4;\n                   ",
         "[employee_table4]",
         "[unknown]",
         "2025-03-26 10:09:36.290130"
        ],
        [
         "31549cd9-c816-4f9f-93da-929500f82fe2",
         "\n                INSERT INTO employee_table4 VALUES\n                (1, 'John Doe', 'Engineering', 1000),\n                (2, 'Jane Smith', 'Marketing', 1200),\n                (3, 'Alice Johnson', 'Finance', 1400),\n                (4, 'Bob Brown', 'Engineering', 1300),\n                (5, 'Charlie White', 'Sales', 1100);\n                ",
         "[]",
         "[employee_table4]",
         "2025-03-26 10:09:31.829305"
        ],
        [
         "3ac6c0ce-9837-4308-980c-2e8bed25004b",
         "\n                CREATE TABLE IF NOT EXISTS employee_table4 (\n                    id INT,\n                    name STRING,\n                    department STRING,\n                    salary INT\n                )\n                USING DELTA;\n                ",
         "[]",
         "[employee_table4]",
         "2025-03-26 10:09:26.634296"
        ],
        [
         "ee6b5b2a-af26-43a3-b38f-edbd1476c377",
         "\n                    SELECT * FROM employee_table4;\n                   ",
         "[employee_table4]",
         "[unknown]",
         "2025-03-26 09:20:49.919760"
        ],
        [
         "9ace40ec-a6b0-4ff2-bddf-1ad9949098ce",
         "\n                INSERT INTO employee_table4 VALUES\n                (1, 'John Doe', 'Engineering', 1000),\n                (2, 'Jane Smith', 'Marketing', 1200),\n                (3, 'Alice Johnson', 'Finance', 1400),\n                (4, 'Bob Brown', 'Engineering', 1300),\n                (5, 'Charlie White', 'Sales', 1100);\n                ",
         "[]",
         "[employee_table4]",
         "2025-03-26 09:20:44.278997"
        ],
        [
         "d1791be4-a5c2-4278-99e2-3c651a3ad85d",
         "\n                CREATE TABLE IF NOT EXISTS employee_table4 (\n                    id INT,\n                    name STRING,\n                    department STRING,\n                    salary INT\n                )\n                USING DELTA;\n                ",
         "[]",
         "[employee_table4]",
         "2025-03-26 09:20:38.031664"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "uuid",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "query",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "source_tables",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "destination_tables",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#sql\n",
    "# Read lineage logs as a Delta table\n",
    "lineage_df = spark.read.format(\"delta\").load(\"/mnt/datalake/Pipeline1/sql/sql_logs/\")\n",
    "\n",
    "# Show all rows without truncation\n",
    "lineage_df.sort(F.col(\"timestamp\").desc()).display(truncate=False, n=lineage_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af309dfb-5573-4186-8954-ad76310d0eb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>uuid</th><th>source</th><th>file_format</th><th>read_parameters</th><th>read_timestamp</th><th>transformation_name</th><th>modification_details</th><th>modification_timestamp</th><th>write_action</th><th>write_details</th><th>write_timestamp</th><th>destination</th></tr></thead><tbody><tr><td>f4694785-cdcd-4087-b193-0ba735363acb</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-03-27 08:05:19.426137</td><td>agg</td><td>{'args': (Column<'sum(amount) AS total_Profit'>,), 'kwargs': {}}</td><td>2025-03-27 08:05:43.319715</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-27 08:06:21.502141</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>f4694785-cdcd-4087-b193-0ba735363acb</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-03-27 08:05:19.426137</td><td>withColumnRenamed</td><td>{'args': ('total_profit', 'sum_profit'), 'kwargs': {}}</td><td>2025-03-27 08:05:47.810618</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-27 08:06:21.502141</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>f4694785-cdcd-4087-b193-0ba735363acb</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-03-27 08:05:19.426137</td><td>orderBy</td><td>{'args': ('sum_profit',), 'kwargs': {'ascending': False}}</td><td>2025-03-27 08:05:51.607075</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-27 08:06:21.502141</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>f4694785-cdcd-4087-b193-0ba735363acb</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-03-27 08:05:19.426137</td><td>distinct</td><td>{'args': (), 'kwargs': {}}</td><td>2025-03-27 08:05:57.024610</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-27 08:06:21.502141</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>f4694785-cdcd-4087-b193-0ba735363acb</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-03-27 08:05:19.426137</td><td>fillna</td><td>{'args': ({'sum_profit': 0},), 'kwargs': {}}</td><td>2025-03-27 08:06:03.098294</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-27 08:06:21.502141</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>f4694785-cdcd-4087-b193-0ba735363acb</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-03-27 08:05:19.426137</td><td>withColumn</td><td>{'args': ('new_column', Column<'static_value'>), 'kwargs': {}}</td><td>2025-03-27 08:06:09.872729</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-27 08:06:21.502141</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>f4694785-cdcd-4087-b193-0ba735363acb</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-03-27 08:05:19.426137</td><td>join</td><td>{'args': (DataFrame[region: string, new_data: bigint],), 'kwargs': {'on': 'region', 'how': 'left'}}</td><td>2025-03-27 08:06:14.416994</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-27 08:06:21.502141</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>82d9db3c-acea-4b91-a14a-da8c33e1930d</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-04-01 09:13:49.349385</td><td>filter</td><td>{'args': ('amount > 100',), 'kwargs': {}}</td><td>2025-04-01 09:13:57.446831</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-04-01 09:14:40.491686</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>82d9db3c-acea-4b91-a14a-da8c33e1930d</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-04-01 09:13:49.349385</td><td>groupBy</td><td>{'args': ('region',), 'kwargs': {}}</td><td>2025-04-01 09:14:03.988241</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-04-01 09:14:40.491686</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>82d9db3c-acea-4b91-a14a-da8c33e1930d</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-04-01 09:13:49.349385</td><td>agg</td><td>{'args': (Column<'sum(amount) AS total_Profit'>,), 'kwargs': {}}</td><td>2025-04-01 09:14:07.235702</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-04-01 09:14:40.491686</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>82d9db3c-acea-4b91-a14a-da8c33e1930d</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-04-01 09:13:49.349385</td><td>withColumnRenamed</td><td>{'args': ('total_profit', 'sum_profit'), 'kwargs': {}}</td><td>2025-04-01 09:14:11.482365</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-04-01 09:14:40.491686</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>82d9db3c-acea-4b91-a14a-da8c33e1930d</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-04-01 09:13:49.349385</td><td>orderBy</td><td>{'args': ('sum_profit',), 'kwargs': {'ascending': False}}</td><td>2025-04-01 09:14:15.300850</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-04-01 09:14:40.491686</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>82d9db3c-acea-4b91-a14a-da8c33e1930d</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-04-01 09:13:49.349385</td><td>distinct</td><td>{'args': (), 'kwargs': {}}</td><td>2025-04-01 09:14:18.927479</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-04-01 09:14:40.491686</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>82d9db3c-acea-4b91-a14a-da8c33e1930d</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-04-01 09:13:49.349385</td><td>fillna</td><td>{'args': ({'sum_profit': 0},), 'kwargs': {}}</td><td>2025-04-01 09:14:23.532534</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-04-01 09:14:40.491686</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>8cebedad-0008-4f4c-9ea3-b0c936237063</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-03-26 09:20:54.757777</td><td>filter</td><td>{'args': ('amount > 100',), 'kwargs': {}}</td><td>2025-03-26 09:20:58.183347</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-26 09:21:28.914155</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>8cebedad-0008-4f4c-9ea3-b0c936237063</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-03-26 09:20:54.757777</td><td>groupBy</td><td>{'args': ('region',), 'kwargs': {}}</td><td>2025-03-26 09:21:01.710217</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-26 09:21:28.914155</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>8cebedad-0008-4f4c-9ea3-b0c936237063</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-03-26 09:20:54.757777</td><td>agg</td><td>{'args': (Column<'sum(amount) AS total_Profit'>,), 'kwargs': {}}</td><td>2025-03-26 09:21:05.321153</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-26 09:21:28.914155</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>8cebedad-0008-4f4c-9ea3-b0c936237063</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-03-26 09:20:54.757777</td><td>withColumnRenamed</td><td>{'args': ('total_profit', 'sum_profit'), 'kwargs': {}}</td><td>2025-03-26 09:21:10.140099</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-26 09:21:28.914155</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>8cebedad-0008-4f4c-9ea3-b0c936237063</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-03-26 09:20:54.757777</td><td>orderBy</td><td>{'args': ('sum_profit',), 'kwargs': {'ascending': False}}</td><td>2025-03-26 09:21:12.320164</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-26 09:21:28.914155</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>8cebedad-0008-4f4c-9ea3-b0c936237063</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-03-26 09:20:54.757777</td><td>distinct</td><td>{'args': (), 'kwargs': {}}</td><td>2025-03-26 09:21:15.117778</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-26 09:21:28.914155</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>8cebedad-0008-4f4c-9ea3-b0c936237063</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-03-26 09:20:54.757777</td><td>fillna</td><td>{'args': ({'sum_profit': 0},), 'kwargs': {}}</td><td>2025-03-26 09:21:18.010204</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-26 09:21:28.914155</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>82d9db3c-acea-4b91-a14a-da8c33e1930d</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-04-01 09:13:49.349385</td><td>withColumn</td><td>{'args': ('new_column', Column<'static_value'>), 'kwargs': {}}</td><td>2025-04-01 09:14:27.203079</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-04-01 09:14:40.491686</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>82d9db3c-acea-4b91-a14a-da8c33e1930d</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-04-01 09:13:49.349385</td><td>join</td><td>{'args': (DataFrame[region: string, new_data: bigint],), 'kwargs': {'on': 'region', 'how': 'left'}}</td><td>2025-04-01 09:14:31.411181</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-04-01 09:14:40.491686</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>82d9db3c-acea-4b91-a14a-da8c33e1930d</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-04-01 09:13:49.349385</td><td>drop</td><td>{'args': ('region',), 'kwargs': {}}</td><td>2025-04-01 09:14:35.524855</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-04-01 09:14:40.491686</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>e97c3ac8-38be-44a2-aa98-9a545a6935ac</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-03-26 10:06:30.070534</td><td>distinct</td><td>{'args': (), 'kwargs': {}}</td><td>2025-03-26 10:06:48.469336</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-26 10:07:04.346735</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>e97c3ac8-38be-44a2-aa98-9a545a6935ac</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-03-26 10:06:30.070534</td><td>fillna</td><td>{'args': ({'sum_profit': 0},), 'kwargs': {}}</td><td>2025-03-26 10:06:51.847327</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-26 10:07:04.346735</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>e97c3ac8-38be-44a2-aa98-9a545a6935ac</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-03-26 10:06:30.070534</td><td>withColumn</td><td>{'args': ('new_column', Column<'static_value'>), 'kwargs': {}}</td><td>2025-03-26 10:06:56.157425</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-26 10:07:04.346735</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>e97c3ac8-38be-44a2-aa98-9a545a6935ac</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-03-26 10:06:30.070534</td><td>join</td><td>{'args': (DataFrame[region: string, new_data: bigint],), 'kwargs': {'on': 'region', 'how': 'left'}}</td><td>2025-03-26 10:06:58.503444</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-26 10:07:04.346735</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>e97c3ac8-38be-44a2-aa98-9a545a6935ac</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-03-26 10:06:30.070534</td><td>drop</td><td>{'args': ('region',), 'kwargs': {}}</td><td>2025-03-26 10:07:01.474868</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-26 10:07:04.346735</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>f4694785-cdcd-4087-b193-0ba735363acb</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-03-27 08:05:19.426137</td><td>filter</td><td>{'args': ('amount > 100',), 'kwargs': {}}</td><td>2025-03-27 08:05:28.890664</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-27 08:06:21.502141</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>f4694785-cdcd-4087-b193-0ba735363acb</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-03-27 08:05:19.426137</td><td>groupBy</td><td>{'args': ('region',), 'kwargs': {}}</td><td>2025-03-27 08:05:37.840896</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-27 08:06:21.502141</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>8cebedad-0008-4f4c-9ea3-b0c936237063</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-03-26 09:20:54.757777</td><td>withColumn</td><td>{'args': ('new_column', Column<'static_value'>), 'kwargs': {}}</td><td>2025-03-26 09:21:20.399158</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-26 09:21:28.914155</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>8cebedad-0008-4f4c-9ea3-b0c936237063</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-03-26 09:20:54.757777</td><td>join</td><td>{'args': (DataFrame[region: string, new_data: bigint],), 'kwargs': {'on': 'region', 'how': 'left'}}</td><td>2025-03-26 09:21:23.386503</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-26 09:21:28.914155</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>8cebedad-0008-4f4c-9ea3-b0c936237063</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-03-26 09:20:54.757777</td><td>drop</td><td>{'args': ('region',), 'kwargs': {}}</td><td>2025-03-26 09:21:26.050580</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-26 09:21:28.914155</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>596b60d8-ffe2-410d-a830-176df59d10c7</td><td>dbfs:/FileStore/files/mtcars.parquet</td><td>parquet</td><td></td><td>2025-03-26 09:21:42.563067</td><td>filter</td><td>{'args': ('cyl > 5',), 'kwargs': {}}</td><td>2025-03-26 09:21:45.749649</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('model', StringType(), True), StructField('mpg', DoubleType(), True), StructField('cyl', IntegerType(), True), StructField('disp', DoubleType(), True), StructField('hp', IntegerType(), True), StructField('drat', DoubleType(), True), StructField('wt', DoubleType(), True), StructField('qsec', DoubleType(), True), StructField('am', IntegerType(), True), StructField('gear', IntegerType(), True), StructField('carb', IntegerType(), True)])\", 'path': '/mnt/datalake/output/Parquetanswer.parquet'}</td><td>2025-03-26 09:21:56.889266</td><td>/mnt/datalake/output/Parquetanswer.parquet</td></tr><tr><td>596b60d8-ffe2-410d-a830-176df59d10c7</td><td>dbfs:/FileStore/files/mtcars.parquet</td><td>parquet</td><td></td><td>2025-03-26 09:21:42.563067</td><td>distinct</td><td>{'args': (), 'kwargs': {}}</td><td>2025-03-26 09:21:48.173015</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('model', StringType(), True), StructField('mpg', DoubleType(), True), StructField('cyl', IntegerType(), True), StructField('disp', DoubleType(), True), StructField('hp', IntegerType(), True), StructField('drat', DoubleType(), True), StructField('wt', DoubleType(), True), StructField('qsec', DoubleType(), True), StructField('am', IntegerType(), True), StructField('gear', IntegerType(), True), StructField('carb', IntegerType(), True)])\", 'path': '/mnt/datalake/output/Parquetanswer.parquet'}</td><td>2025-03-26 09:21:56.889266</td><td>/mnt/datalake/output/Parquetanswer.parquet</td></tr><tr><td>596b60d8-ffe2-410d-a830-176df59d10c7</td><td>dbfs:/FileStore/files/mtcars.parquet</td><td>parquet</td><td></td><td>2025-03-26 09:21:42.563067</td><td>drop</td><td>{'args': ('vs',), 'kwargs': {}}</td><td>2025-03-26 09:21:52.905821</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('model', StringType(), True), StructField('mpg', DoubleType(), True), StructField('cyl', IntegerType(), True), StructField('disp', DoubleType(), True), StructField('hp', IntegerType(), True), StructField('drat', DoubleType(), True), StructField('wt', DoubleType(), True), StructField('qsec', DoubleType(), True), StructField('am', IntegerType(), True), StructField('gear', IntegerType(), True), StructField('carb', IntegerType(), True)])\", 'path': '/mnt/datalake/output/Parquetanswer.parquet'}</td><td>2025-03-26 09:21:56.889266</td><td>/mnt/datalake/output/Parquetanswer.parquet</td></tr><tr><td>20bdfccc-fa44-4218-a391-1019371af7d9</td><td>dbfs:/tmp/sample_data.avro</td><td>avro</td><td>{'header': True}</td><td>2025-03-26 09:22:09.736725</td><td>filter</td><td>{'args': ('Age> 5',), 'kwargs': {}}</td><td>2025-03-26 09:22:13.737748</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('ID', IntegerType(), True), StructField('Name', StringType(), True), StructField('Age', IntegerType(), True), StructField('Country', StringType(), True), StructField('Salary', FloatType(), True)])\", 'path': '/mnt/datalake/output/Avroanswer.parquet'}</td><td>2025-03-26 09:22:22.331375</td><td>/mnt/datalake/output/Avroanswer.parquet</td></tr><tr><td>20bdfccc-fa44-4218-a391-1019371af7d9</td><td>dbfs:/tmp/sample_data.avro</td><td>avro</td><td>{'header': True}</td><td>2025-03-26 09:22:09.736725</td><td>distinct</td><td>{'args': (), 'kwargs': {}}</td><td>2025-03-26 09:22:17.225162</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('ID', IntegerType(), True), StructField('Name', StringType(), True), StructField('Age', IntegerType(), True), StructField('Country', StringType(), True), StructField('Salary', FloatType(), True)])\", 'path': '/mnt/datalake/output/Avroanswer.parquet'}</td><td>2025-03-26 09:22:22.331375</td><td>/mnt/datalake/output/Avroanswer.parquet</td></tr><tr><td>20bdfccc-fa44-4218-a391-1019371af7d9</td><td>dbfs:/tmp/sample_data.avro</td><td>avro</td><td>{'header': True}</td><td>2025-03-26 09:22:09.736725</td><td>drop</td><td>{'args': ('JoiningDate',), 'kwargs': {}}</td><td>2025-03-26 09:22:19.878768</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('ID', IntegerType(), True), StructField('Name', StringType(), True), StructField('Age', IntegerType(), True), StructField('Country', StringType(), True), StructField('Salary', FloatType(), True)])\", 'path': '/mnt/datalake/output/Avroanswer.parquet'}</td><td>2025-03-26 09:22:22.331375</td><td>/mnt/datalake/output/Avroanswer.parquet</td></tr><tr><td>e97c3ac8-38be-44a2-aa98-9a545a6935ac</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-03-26 10:06:30.070534</td><td>filter</td><td>{'args': ('amount > 100',), 'kwargs': {}}</td><td>2025-03-26 10:06:34.038897</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-26 10:07:04.346735</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>e97c3ac8-38be-44a2-aa98-9a545a6935ac</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-03-26 10:06:30.070534</td><td>groupBy</td><td>{'args': ('region',), 'kwargs': {}}</td><td>2025-03-26 10:06:36.555283</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-26 10:07:04.346735</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>e97c3ac8-38be-44a2-aa98-9a545a6935ac</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-03-26 10:06:30.070534</td><td>agg</td><td>{'args': (Column<'sum(amount) AS total_Profit'>,), 'kwargs': {}}</td><td>2025-03-26 10:06:39.724818</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-26 10:07:04.346735</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>e97c3ac8-38be-44a2-aa98-9a545a6935ac</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-03-26 10:06:30.070534</td><td>withColumnRenamed</td><td>{'args': ('total_profit', 'sum_profit'), 'kwargs': {}}</td><td>2025-03-26 10:06:42.461104</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-26 10:07:04.346735</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>e97c3ac8-38be-44a2-aa98-9a545a6935ac</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-03-26 10:06:30.070534</td><td>orderBy</td><td>{'args': ('sum_profit',), 'kwargs': {'ascending': False}}</td><td>2025-03-26 10:06:45.149999</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-26 10:07:04.346735</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>f4694785-cdcd-4087-b193-0ba735363acb</td><td>dbfs:/FileStore/files/source_data.csv</td><td>csv</td><td>{'header': True}</td><td>2025-03-27 08:05:19.426137</td><td>drop</td><td>{'args': ('region',), 'kwargs': {}}</td><td>2025-03-27 08:06:18.256485</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}</td><td>2025-03-27 08:06:21.502141</td><td>/mnt/datalake/output/answer.parquet</td></tr><tr><td>2706e19d-44f7-414c-aecc-6ff1a614351d</td><td>dbfs:/FileStore/files/mtcars.parquet</td><td>parquet</td><td></td><td>2025-03-27 08:06:54.407705</td><td>filter</td><td>{'args': ('cyl > 5',), 'kwargs': {}}</td><td>2025-03-27 08:06:59.482955</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('model', StringType(), True), StructField('mpg', DoubleType(), True), StructField('cyl', IntegerType(), True), StructField('disp', DoubleType(), True), StructField('hp', IntegerType(), True), StructField('drat', DoubleType(), True), StructField('wt', DoubleType(), True), StructField('qsec', DoubleType(), True), StructField('am', IntegerType(), True), StructField('gear', IntegerType(), True), StructField('carb', IntegerType(), True)])\", 'path': '/mnt/datalake/output/Parquetanswer.parquet'}</td><td>2025-03-27 08:07:13.247953</td><td>/mnt/datalake/output/Parquetanswer.parquet</td></tr><tr><td>2706e19d-44f7-414c-aecc-6ff1a614351d</td><td>dbfs:/FileStore/files/mtcars.parquet</td><td>parquet</td><td></td><td>2025-03-27 08:06:54.407705</td><td>distinct</td><td>{'args': (), 'kwargs': {}}</td><td>2025-03-27 08:07:03.627471</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('model', StringType(), True), StructField('mpg', DoubleType(), True), StructField('cyl', IntegerType(), True), StructField('disp', DoubleType(), True), StructField('hp', IntegerType(), True), StructField('drat', DoubleType(), True), StructField('wt', DoubleType(), True), StructField('qsec', DoubleType(), True), StructField('am', IntegerType(), True), StructField('gear', IntegerType(), True), StructField('carb', IntegerType(), True)])\", 'path': '/mnt/datalake/output/Parquetanswer.parquet'}</td><td>2025-03-27 08:07:13.247953</td><td>/mnt/datalake/output/Parquetanswer.parquet</td></tr><tr><td>2706e19d-44f7-414c-aecc-6ff1a614351d</td><td>dbfs:/FileStore/files/mtcars.parquet</td><td>parquet</td><td></td><td>2025-03-27 08:06:54.407705</td><td>drop</td><td>{'args': ('vs',), 'kwargs': {}}</td><td>2025-03-27 08:07:08.096207</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('model', StringType(), True), StructField('mpg', DoubleType(), True), StructField('cyl', IntegerType(), True), StructField('disp', DoubleType(), True), StructField('hp', IntegerType(), True), StructField('drat', DoubleType(), True), StructField('wt', DoubleType(), True), StructField('qsec', DoubleType(), True), StructField('am', IntegerType(), True), StructField('gear', IntegerType(), True), StructField('carb', IntegerType(), True)])\", 'path': '/mnt/datalake/output/Parquetanswer.parquet'}</td><td>2025-03-27 08:07:13.247953</td><td>/mnt/datalake/output/Parquetanswer.parquet</td></tr><tr><td>0679e0e6-b0b7-45d5-82e3-0d22f8d9686f</td><td>dbfs:/tmp/sample_data.avro</td><td>avro</td><td>{'header': True}</td><td>2025-03-27 08:07:38.735051</td><td>filter</td><td>{'args': ('Age> 5',), 'kwargs': {}}</td><td>2025-03-27 08:07:43.698757</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('ID', IntegerType(), True), StructField('Name', StringType(), True), StructField('Age', IntegerType(), True), StructField('Country', StringType(), True), StructField('Salary', FloatType(), True)])\", 'path': '/mnt/datalake/output/Avroanswer.parquet'}</td><td>2025-03-27 08:07:57.835065</td><td>/mnt/datalake/output/Avroanswer.parquet</td></tr><tr><td>0679e0e6-b0b7-45d5-82e3-0d22f8d9686f</td><td>dbfs:/tmp/sample_data.avro</td><td>avro</td><td>{'header': True}</td><td>2025-03-27 08:07:38.735051</td><td>distinct</td><td>{'args': (), 'kwargs': {}}</td><td>2025-03-27 08:07:48.294226</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('ID', IntegerType(), True), StructField('Name', StringType(), True), StructField('Age', IntegerType(), True), StructField('Country', StringType(), True), StructField('Salary', FloatType(), True)])\", 'path': '/mnt/datalake/output/Avroanswer.parquet'}</td><td>2025-03-27 08:07:57.835065</td><td>/mnt/datalake/output/Avroanswer.parquet</td></tr><tr><td>0679e0e6-b0b7-45d5-82e3-0d22f8d9686f</td><td>dbfs:/tmp/sample_data.avro</td><td>avro</td><td>{'header': True}</td><td>2025-03-27 08:07:38.735051</td><td>drop</td><td>{'args': ('JoiningDate',), 'kwargs': {}}</td><td>2025-03-27 08:07:52.069528</td><td>line_write</td><td>{'mode': 'overwrite', 'df_schema': \"StructType([StructField('ID', IntegerType(), True), StructField('Name', StringType(), True), StructField('Age', IntegerType(), True), StructField('Country', StringType(), True), StructField('Salary', FloatType(), True)])\", 'path': '/mnt/datalake/output/Avroanswer.parquet'}</td><td>2025-03-27 08:07:57.835065</td><td>/mnt/datalake/output/Avroanswer.parquet</td></tr><tr><td>null</td><td>[]</td><td>null</td><td>null</td><td>null</td><td>null</td><td>\n",
       "                CREATE TABLE IF NOT EXISTS employee_table5 (\n",
       "                    id INT,\n",
       "                    name STRING,\n",
       "                    department STRING,\n",
       "                    salary INT\n",
       "                )\n",
       "                USING DELTA;\n",
       "                </td><td>2025-04-01 09:13:28.039220</td><td>null</td><td>null</td><td>null</td><td>[employee_table5]</td></tr><tr><td>null</td><td>[]</td><td>null</td><td>null</td><td>null</td><td>null</td><td>\n",
       "                CREATE TABLE IF NOT EXISTS employee_table4 (\n",
       "                    id INT,\n",
       "                    name STRING,\n",
       "                    department STRING,\n",
       "                    salary INT\n",
       "                )\n",
       "                USING DELTA;\n",
       "                </td><td>2025-03-26 09:20:38.031664</td><td>null</td><td>null</td><td>null</td><td>[employee_table4]</td></tr><tr><td>null</td><td>[]</td><td>null</td><td>null</td><td>null</td><td>null</td><td>\n",
       "                INSERT INTO employee_table4 VALUES\n",
       "                (1, 'John Doe', 'Engineering', 1000),\n",
       "                (2, 'Jane Smith', 'Marketing', 1200),\n",
       "                (3, 'Alice Johnson', 'Finance', 1400),\n",
       "                (4, 'Bob Brown', 'Engineering', 1300),\n",
       "                (5, 'Charlie White', 'Sales', 1100);\n",
       "                </td><td>2025-03-26 09:20:44.278997</td><td>null</td><td>null</td><td>null</td><td>[employee_table4]</td></tr><tr><td>null</td><td>[]</td><td>null</td><td>null</td><td>null</td><td>null</td><td>\n",
       "                CREATE TABLE IF NOT EXISTS employee_table4 (\n",
       "                    id INT,\n",
       "                    name STRING,\n",
       "                    department STRING,\n",
       "                    salary INT\n",
       "                )\n",
       "                USING DELTA;\n",
       "                </td><td>2025-03-27 08:03:19.809104</td><td>null</td><td>null</td><td>null</td><td>[employee_table4]</td></tr><tr><td>null</td><td>[]</td><td>null</td><td>null</td><td>null</td><td>null</td><td>\n",
       "                CREATE TABLE IF NOT EXISTS employee_table5 (\n",
       "                    id INT,\n",
       "                    name STRING,\n",
       "                    department STRING,\n",
       "                    salary INT\n",
       "                )\n",
       "                USING DELTA;\n",
       "                </td><td>2025-03-27 08:04:49.613385</td><td>null</td><td>null</td><td>null</td><td>[employee_table5]</td></tr><tr><td>null</td><td>[]</td><td>null</td><td>null</td><td>null</td><td>null</td><td>\n",
       "                INSERT INTO employee_table5 VALUES\n",
       "                (1, 'John Doe', 'Engineering', 1000),\n",
       "                (2, 'Jane Smith', 'Marketing', 1200),\n",
       "                (3, 'Alice Johnson', 'Finance', 1400),\n",
       "                (4, 'Bob Brown', 'Engineering', 1300),\n",
       "                (5, 'Charlie White', 'Sales', 1100);\n",
       "                </td><td>2025-03-27 08:05:01.238966</td><td>null</td><td>null</td><td>null</td><td>[employee_table5]</td></tr><tr><td>null</td><td>[employee_table5]</td><td>null</td><td>null</td><td>null</td><td>null</td><td>\n",
       "                    SELECT * FROM employee_table5;\n",
       "                   </td><td>2025-03-27 08:05:12.680028</td><td>null</td><td>null</td><td>null</td><td>[unknown]</td></tr><tr><td>null</td><td>[]</td><td>null</td><td>null</td><td>null</td><td>null</td><td>\n",
       "                INSERT INTO employee_table4 VALUES\n",
       "                (1, 'John Doe', 'Engineering', 1000),\n",
       "                (2, 'Jane Smith', 'Marketing', 1200),\n",
       "                (3, 'Alice Johnson', 'Finance', 1400),\n",
       "                (4, 'Bob Brown', 'Engineering', 1300),\n",
       "                (5, 'Charlie White', 'Sales', 1100);\n",
       "                </td><td>2025-03-26 10:09:31.829305</td><td>null</td><td>null</td><td>null</td><td>[employee_table4]</td></tr><tr><td>null</td><td>[employee_table4]</td><td>null</td><td>null</td><td>null</td><td>null</td><td>\n",
       "                    SELECT * FROM employee_table4;\n",
       "                   </td><td>2025-03-26 10:09:36.290130</td><td>null</td><td>null</td><td>null</td><td>[unknown]</td></tr><tr><td>null</td><td>[employee_table4]</td><td>null</td><td>null</td><td>null</td><td>null</td><td>\n",
       "                    SELECT * FROM employee_table4;\n",
       "                   </td><td>2025-03-26 09:20:49.919760</td><td>null</td><td>null</td><td>null</td><td>[unknown]</td></tr><tr><td>null</td><td>[]</td><td>null</td><td>null</td><td>null</td><td>null</td><td>\n",
       "                CREATE TABLE IF NOT EXISTS employee_table4 (\n",
       "                    id INT,\n",
       "                    name STRING,\n",
       "                    department STRING,\n",
       "                    salary INT\n",
       "                )\n",
       "                USING DELTA;\n",
       "                </td><td>2025-03-26 10:09:26.634296</td><td>null</td><td>null</td><td>null</td><td>[employee_table4]</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "f4694785-cdcd-4087-b193-0ba735363acb",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-03-27 08:05:19.426137",
         "agg",
         "{'args': (Column<'sum(amount) AS total_Profit'>,), 'kwargs': {}}",
         "2025-03-27 08:05:43.319715",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-27 08:06:21.502141",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "f4694785-cdcd-4087-b193-0ba735363acb",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-03-27 08:05:19.426137",
         "withColumnRenamed",
         "{'args': ('total_profit', 'sum_profit'), 'kwargs': {}}",
         "2025-03-27 08:05:47.810618",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-27 08:06:21.502141",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "f4694785-cdcd-4087-b193-0ba735363acb",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-03-27 08:05:19.426137",
         "orderBy",
         "{'args': ('sum_profit',), 'kwargs': {'ascending': False}}",
         "2025-03-27 08:05:51.607075",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-27 08:06:21.502141",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "f4694785-cdcd-4087-b193-0ba735363acb",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-03-27 08:05:19.426137",
         "distinct",
         "{'args': (), 'kwargs': {}}",
         "2025-03-27 08:05:57.024610",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-27 08:06:21.502141",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "f4694785-cdcd-4087-b193-0ba735363acb",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-03-27 08:05:19.426137",
         "fillna",
         "{'args': ({'sum_profit': 0},), 'kwargs': {}}",
         "2025-03-27 08:06:03.098294",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-27 08:06:21.502141",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "f4694785-cdcd-4087-b193-0ba735363acb",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-03-27 08:05:19.426137",
         "withColumn",
         "{'args': ('new_column', Column<'static_value'>), 'kwargs': {}}",
         "2025-03-27 08:06:09.872729",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-27 08:06:21.502141",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "f4694785-cdcd-4087-b193-0ba735363acb",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-03-27 08:05:19.426137",
         "join",
         "{'args': (DataFrame[region: string, new_data: bigint],), 'kwargs': {'on': 'region', 'how': 'left'}}",
         "2025-03-27 08:06:14.416994",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-27 08:06:21.502141",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "82d9db3c-acea-4b91-a14a-da8c33e1930d",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-04-01 09:13:49.349385",
         "filter",
         "{'args': ('amount > 100',), 'kwargs': {}}",
         "2025-04-01 09:13:57.446831",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-04-01 09:14:40.491686",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "82d9db3c-acea-4b91-a14a-da8c33e1930d",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-04-01 09:13:49.349385",
         "groupBy",
         "{'args': ('region',), 'kwargs': {}}",
         "2025-04-01 09:14:03.988241",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-04-01 09:14:40.491686",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "82d9db3c-acea-4b91-a14a-da8c33e1930d",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-04-01 09:13:49.349385",
         "agg",
         "{'args': (Column<'sum(amount) AS total_Profit'>,), 'kwargs': {}}",
         "2025-04-01 09:14:07.235702",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-04-01 09:14:40.491686",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "82d9db3c-acea-4b91-a14a-da8c33e1930d",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-04-01 09:13:49.349385",
         "withColumnRenamed",
         "{'args': ('total_profit', 'sum_profit'), 'kwargs': {}}",
         "2025-04-01 09:14:11.482365",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-04-01 09:14:40.491686",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "82d9db3c-acea-4b91-a14a-da8c33e1930d",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-04-01 09:13:49.349385",
         "orderBy",
         "{'args': ('sum_profit',), 'kwargs': {'ascending': False}}",
         "2025-04-01 09:14:15.300850",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-04-01 09:14:40.491686",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "82d9db3c-acea-4b91-a14a-da8c33e1930d",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-04-01 09:13:49.349385",
         "distinct",
         "{'args': (), 'kwargs': {}}",
         "2025-04-01 09:14:18.927479",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-04-01 09:14:40.491686",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "82d9db3c-acea-4b91-a14a-da8c33e1930d",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-04-01 09:13:49.349385",
         "fillna",
         "{'args': ({'sum_profit': 0},), 'kwargs': {}}",
         "2025-04-01 09:14:23.532534",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-04-01 09:14:40.491686",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "8cebedad-0008-4f4c-9ea3-b0c936237063",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-03-26 09:20:54.757777",
         "filter",
         "{'args': ('amount > 100',), 'kwargs': {}}",
         "2025-03-26 09:20:58.183347",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-26 09:21:28.914155",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "8cebedad-0008-4f4c-9ea3-b0c936237063",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-03-26 09:20:54.757777",
         "groupBy",
         "{'args': ('region',), 'kwargs': {}}",
         "2025-03-26 09:21:01.710217",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-26 09:21:28.914155",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "8cebedad-0008-4f4c-9ea3-b0c936237063",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-03-26 09:20:54.757777",
         "agg",
         "{'args': (Column<'sum(amount) AS total_Profit'>,), 'kwargs': {}}",
         "2025-03-26 09:21:05.321153",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-26 09:21:28.914155",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "8cebedad-0008-4f4c-9ea3-b0c936237063",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-03-26 09:20:54.757777",
         "withColumnRenamed",
         "{'args': ('total_profit', 'sum_profit'), 'kwargs': {}}",
         "2025-03-26 09:21:10.140099",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-26 09:21:28.914155",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "8cebedad-0008-4f4c-9ea3-b0c936237063",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-03-26 09:20:54.757777",
         "orderBy",
         "{'args': ('sum_profit',), 'kwargs': {'ascending': False}}",
         "2025-03-26 09:21:12.320164",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-26 09:21:28.914155",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "8cebedad-0008-4f4c-9ea3-b0c936237063",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-03-26 09:20:54.757777",
         "distinct",
         "{'args': (), 'kwargs': {}}",
         "2025-03-26 09:21:15.117778",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-26 09:21:28.914155",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "8cebedad-0008-4f4c-9ea3-b0c936237063",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-03-26 09:20:54.757777",
         "fillna",
         "{'args': ({'sum_profit': 0},), 'kwargs': {}}",
         "2025-03-26 09:21:18.010204",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-26 09:21:28.914155",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "82d9db3c-acea-4b91-a14a-da8c33e1930d",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-04-01 09:13:49.349385",
         "withColumn",
         "{'args': ('new_column', Column<'static_value'>), 'kwargs': {}}",
         "2025-04-01 09:14:27.203079",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-04-01 09:14:40.491686",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "82d9db3c-acea-4b91-a14a-da8c33e1930d",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-04-01 09:13:49.349385",
         "join",
         "{'args': (DataFrame[region: string, new_data: bigint],), 'kwargs': {'on': 'region', 'how': 'left'}}",
         "2025-04-01 09:14:31.411181",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-04-01 09:14:40.491686",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "82d9db3c-acea-4b91-a14a-da8c33e1930d",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-04-01 09:13:49.349385",
         "drop",
         "{'args': ('region',), 'kwargs': {}}",
         "2025-04-01 09:14:35.524855",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-04-01 09:14:40.491686",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "e97c3ac8-38be-44a2-aa98-9a545a6935ac",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-03-26 10:06:30.070534",
         "distinct",
         "{'args': (), 'kwargs': {}}",
         "2025-03-26 10:06:48.469336",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-26 10:07:04.346735",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "e97c3ac8-38be-44a2-aa98-9a545a6935ac",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-03-26 10:06:30.070534",
         "fillna",
         "{'args': ({'sum_profit': 0},), 'kwargs': {}}",
         "2025-03-26 10:06:51.847327",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-26 10:07:04.346735",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "e97c3ac8-38be-44a2-aa98-9a545a6935ac",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-03-26 10:06:30.070534",
         "withColumn",
         "{'args': ('new_column', Column<'static_value'>), 'kwargs': {}}",
         "2025-03-26 10:06:56.157425",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-26 10:07:04.346735",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "e97c3ac8-38be-44a2-aa98-9a545a6935ac",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-03-26 10:06:30.070534",
         "join",
         "{'args': (DataFrame[region: string, new_data: bigint],), 'kwargs': {'on': 'region', 'how': 'left'}}",
         "2025-03-26 10:06:58.503444",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-26 10:07:04.346735",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "e97c3ac8-38be-44a2-aa98-9a545a6935ac",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-03-26 10:06:30.070534",
         "drop",
         "{'args': ('region',), 'kwargs': {}}",
         "2025-03-26 10:07:01.474868",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-26 10:07:04.346735",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "f4694785-cdcd-4087-b193-0ba735363acb",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-03-27 08:05:19.426137",
         "filter",
         "{'args': ('amount > 100',), 'kwargs': {}}",
         "2025-03-27 08:05:28.890664",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-27 08:06:21.502141",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "f4694785-cdcd-4087-b193-0ba735363acb",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-03-27 08:05:19.426137",
         "groupBy",
         "{'args': ('region',), 'kwargs': {}}",
         "2025-03-27 08:05:37.840896",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-27 08:06:21.502141",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "8cebedad-0008-4f4c-9ea3-b0c936237063",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-03-26 09:20:54.757777",
         "withColumn",
         "{'args': ('new_column', Column<'static_value'>), 'kwargs': {}}",
         "2025-03-26 09:21:20.399158",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-26 09:21:28.914155",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "8cebedad-0008-4f4c-9ea3-b0c936237063",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-03-26 09:20:54.757777",
         "join",
         "{'args': (DataFrame[region: string, new_data: bigint],), 'kwargs': {'on': 'region', 'how': 'left'}}",
         "2025-03-26 09:21:23.386503",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-26 09:21:28.914155",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "8cebedad-0008-4f4c-9ea3-b0c936237063",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-03-26 09:20:54.757777",
         "drop",
         "{'args': ('region',), 'kwargs': {}}",
         "2025-03-26 09:21:26.050580",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-26 09:21:28.914155",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "596b60d8-ffe2-410d-a830-176df59d10c7",
         "dbfs:/FileStore/files/mtcars.parquet",
         "parquet",
         "",
         "2025-03-26 09:21:42.563067",
         "filter",
         "{'args': ('cyl > 5',), 'kwargs': {}}",
         "2025-03-26 09:21:45.749649",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('model', StringType(), True), StructField('mpg', DoubleType(), True), StructField('cyl', IntegerType(), True), StructField('disp', DoubleType(), True), StructField('hp', IntegerType(), True), StructField('drat', DoubleType(), True), StructField('wt', DoubleType(), True), StructField('qsec', DoubleType(), True), StructField('am', IntegerType(), True), StructField('gear', IntegerType(), True), StructField('carb', IntegerType(), True)])\", 'path': '/mnt/datalake/output/Parquetanswer.parquet'}",
         "2025-03-26 09:21:56.889266",
         "/mnt/datalake/output/Parquetanswer.parquet"
        ],
        [
         "596b60d8-ffe2-410d-a830-176df59d10c7",
         "dbfs:/FileStore/files/mtcars.parquet",
         "parquet",
         "",
         "2025-03-26 09:21:42.563067",
         "distinct",
         "{'args': (), 'kwargs': {}}",
         "2025-03-26 09:21:48.173015",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('model', StringType(), True), StructField('mpg', DoubleType(), True), StructField('cyl', IntegerType(), True), StructField('disp', DoubleType(), True), StructField('hp', IntegerType(), True), StructField('drat', DoubleType(), True), StructField('wt', DoubleType(), True), StructField('qsec', DoubleType(), True), StructField('am', IntegerType(), True), StructField('gear', IntegerType(), True), StructField('carb', IntegerType(), True)])\", 'path': '/mnt/datalake/output/Parquetanswer.parquet'}",
         "2025-03-26 09:21:56.889266",
         "/mnt/datalake/output/Parquetanswer.parquet"
        ],
        [
         "596b60d8-ffe2-410d-a830-176df59d10c7",
         "dbfs:/FileStore/files/mtcars.parquet",
         "parquet",
         "",
         "2025-03-26 09:21:42.563067",
         "drop",
         "{'args': ('vs',), 'kwargs': {}}",
         "2025-03-26 09:21:52.905821",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('model', StringType(), True), StructField('mpg', DoubleType(), True), StructField('cyl', IntegerType(), True), StructField('disp', DoubleType(), True), StructField('hp', IntegerType(), True), StructField('drat', DoubleType(), True), StructField('wt', DoubleType(), True), StructField('qsec', DoubleType(), True), StructField('am', IntegerType(), True), StructField('gear', IntegerType(), True), StructField('carb', IntegerType(), True)])\", 'path': '/mnt/datalake/output/Parquetanswer.parquet'}",
         "2025-03-26 09:21:56.889266",
         "/mnt/datalake/output/Parquetanswer.parquet"
        ],
        [
         "20bdfccc-fa44-4218-a391-1019371af7d9",
         "dbfs:/tmp/sample_data.avro",
         "avro",
         "{'header': True}",
         "2025-03-26 09:22:09.736725",
         "filter",
         "{'args': ('Age> 5',), 'kwargs': {}}",
         "2025-03-26 09:22:13.737748",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('ID', IntegerType(), True), StructField('Name', StringType(), True), StructField('Age', IntegerType(), True), StructField('Country', StringType(), True), StructField('Salary', FloatType(), True)])\", 'path': '/mnt/datalake/output/Avroanswer.parquet'}",
         "2025-03-26 09:22:22.331375",
         "/mnt/datalake/output/Avroanswer.parquet"
        ],
        [
         "20bdfccc-fa44-4218-a391-1019371af7d9",
         "dbfs:/tmp/sample_data.avro",
         "avro",
         "{'header': True}",
         "2025-03-26 09:22:09.736725",
         "distinct",
         "{'args': (), 'kwargs': {}}",
         "2025-03-26 09:22:17.225162",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('ID', IntegerType(), True), StructField('Name', StringType(), True), StructField('Age', IntegerType(), True), StructField('Country', StringType(), True), StructField('Salary', FloatType(), True)])\", 'path': '/mnt/datalake/output/Avroanswer.parquet'}",
         "2025-03-26 09:22:22.331375",
         "/mnt/datalake/output/Avroanswer.parquet"
        ],
        [
         "20bdfccc-fa44-4218-a391-1019371af7d9",
         "dbfs:/tmp/sample_data.avro",
         "avro",
         "{'header': True}",
         "2025-03-26 09:22:09.736725",
         "drop",
         "{'args': ('JoiningDate',), 'kwargs': {}}",
         "2025-03-26 09:22:19.878768",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('ID', IntegerType(), True), StructField('Name', StringType(), True), StructField('Age', IntegerType(), True), StructField('Country', StringType(), True), StructField('Salary', FloatType(), True)])\", 'path': '/mnt/datalake/output/Avroanswer.parquet'}",
         "2025-03-26 09:22:22.331375",
         "/mnt/datalake/output/Avroanswer.parquet"
        ],
        [
         "e97c3ac8-38be-44a2-aa98-9a545a6935ac",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-03-26 10:06:30.070534",
         "filter",
         "{'args': ('amount > 100',), 'kwargs': {}}",
         "2025-03-26 10:06:34.038897",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-26 10:07:04.346735",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "e97c3ac8-38be-44a2-aa98-9a545a6935ac",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-03-26 10:06:30.070534",
         "groupBy",
         "{'args': ('region',), 'kwargs': {}}",
         "2025-03-26 10:06:36.555283",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-26 10:07:04.346735",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "e97c3ac8-38be-44a2-aa98-9a545a6935ac",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-03-26 10:06:30.070534",
         "agg",
         "{'args': (Column<'sum(amount) AS total_Profit'>,), 'kwargs': {}}",
         "2025-03-26 10:06:39.724818",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-26 10:07:04.346735",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "e97c3ac8-38be-44a2-aa98-9a545a6935ac",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-03-26 10:06:30.070534",
         "withColumnRenamed",
         "{'args': ('total_profit', 'sum_profit'), 'kwargs': {}}",
         "2025-03-26 10:06:42.461104",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-26 10:07:04.346735",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "e97c3ac8-38be-44a2-aa98-9a545a6935ac",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-03-26 10:06:30.070534",
         "orderBy",
         "{'args': ('sum_profit',), 'kwargs': {'ascending': False}}",
         "2025-03-26 10:06:45.149999",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-26 10:07:04.346735",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "f4694785-cdcd-4087-b193-0ba735363acb",
         "dbfs:/FileStore/files/source_data.csv",
         "csv",
         "{'header': True}",
         "2025-03-27 08:05:19.426137",
         "drop",
         "{'args': ('region',), 'kwargs': {}}",
         "2025-03-27 08:06:18.256485",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('sum_profit', DoubleType(), False), StructField('new_column', StringType(), False), StructField('new_data', LongType(), True)])\", 'path': '/mnt/datalake/output/answer.parquet'}",
         "2025-03-27 08:06:21.502141",
         "/mnt/datalake/output/answer.parquet"
        ],
        [
         "2706e19d-44f7-414c-aecc-6ff1a614351d",
         "dbfs:/FileStore/files/mtcars.parquet",
         "parquet",
         "",
         "2025-03-27 08:06:54.407705",
         "filter",
         "{'args': ('cyl > 5',), 'kwargs': {}}",
         "2025-03-27 08:06:59.482955",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('model', StringType(), True), StructField('mpg', DoubleType(), True), StructField('cyl', IntegerType(), True), StructField('disp', DoubleType(), True), StructField('hp', IntegerType(), True), StructField('drat', DoubleType(), True), StructField('wt', DoubleType(), True), StructField('qsec', DoubleType(), True), StructField('am', IntegerType(), True), StructField('gear', IntegerType(), True), StructField('carb', IntegerType(), True)])\", 'path': '/mnt/datalake/output/Parquetanswer.parquet'}",
         "2025-03-27 08:07:13.247953",
         "/mnt/datalake/output/Parquetanswer.parquet"
        ],
        [
         "2706e19d-44f7-414c-aecc-6ff1a614351d",
         "dbfs:/FileStore/files/mtcars.parquet",
         "parquet",
         "",
         "2025-03-27 08:06:54.407705",
         "distinct",
         "{'args': (), 'kwargs': {}}",
         "2025-03-27 08:07:03.627471",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('model', StringType(), True), StructField('mpg', DoubleType(), True), StructField('cyl', IntegerType(), True), StructField('disp', DoubleType(), True), StructField('hp', IntegerType(), True), StructField('drat', DoubleType(), True), StructField('wt', DoubleType(), True), StructField('qsec', DoubleType(), True), StructField('am', IntegerType(), True), StructField('gear', IntegerType(), True), StructField('carb', IntegerType(), True)])\", 'path': '/mnt/datalake/output/Parquetanswer.parquet'}",
         "2025-03-27 08:07:13.247953",
         "/mnt/datalake/output/Parquetanswer.parquet"
        ],
        [
         "2706e19d-44f7-414c-aecc-6ff1a614351d",
         "dbfs:/FileStore/files/mtcars.parquet",
         "parquet",
         "",
         "2025-03-27 08:06:54.407705",
         "drop",
         "{'args': ('vs',), 'kwargs': {}}",
         "2025-03-27 08:07:08.096207",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('model', StringType(), True), StructField('mpg', DoubleType(), True), StructField('cyl', IntegerType(), True), StructField('disp', DoubleType(), True), StructField('hp', IntegerType(), True), StructField('drat', DoubleType(), True), StructField('wt', DoubleType(), True), StructField('qsec', DoubleType(), True), StructField('am', IntegerType(), True), StructField('gear', IntegerType(), True), StructField('carb', IntegerType(), True)])\", 'path': '/mnt/datalake/output/Parquetanswer.parquet'}",
         "2025-03-27 08:07:13.247953",
         "/mnt/datalake/output/Parquetanswer.parquet"
        ],
        [
         "0679e0e6-b0b7-45d5-82e3-0d22f8d9686f",
         "dbfs:/tmp/sample_data.avro",
         "avro",
         "{'header': True}",
         "2025-03-27 08:07:38.735051",
         "filter",
         "{'args': ('Age> 5',), 'kwargs': {}}",
         "2025-03-27 08:07:43.698757",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('ID', IntegerType(), True), StructField('Name', StringType(), True), StructField('Age', IntegerType(), True), StructField('Country', StringType(), True), StructField('Salary', FloatType(), True)])\", 'path': '/mnt/datalake/output/Avroanswer.parquet'}",
         "2025-03-27 08:07:57.835065",
         "/mnt/datalake/output/Avroanswer.parquet"
        ],
        [
         "0679e0e6-b0b7-45d5-82e3-0d22f8d9686f",
         "dbfs:/tmp/sample_data.avro",
         "avro",
         "{'header': True}",
         "2025-03-27 08:07:38.735051",
         "distinct",
         "{'args': (), 'kwargs': {}}",
         "2025-03-27 08:07:48.294226",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('ID', IntegerType(), True), StructField('Name', StringType(), True), StructField('Age', IntegerType(), True), StructField('Country', StringType(), True), StructField('Salary', FloatType(), True)])\", 'path': '/mnt/datalake/output/Avroanswer.parquet'}",
         "2025-03-27 08:07:57.835065",
         "/mnt/datalake/output/Avroanswer.parquet"
        ],
        [
         "0679e0e6-b0b7-45d5-82e3-0d22f8d9686f",
         "dbfs:/tmp/sample_data.avro",
         "avro",
         "{'header': True}",
         "2025-03-27 08:07:38.735051",
         "drop",
         "{'args': ('JoiningDate',), 'kwargs': {}}",
         "2025-03-27 08:07:52.069528",
         "line_write",
         "{'mode': 'overwrite', 'df_schema': \"StructType([StructField('ID', IntegerType(), True), StructField('Name', StringType(), True), StructField('Age', IntegerType(), True), StructField('Country', StringType(), True), StructField('Salary', FloatType(), True)])\", 'path': '/mnt/datalake/output/Avroanswer.parquet'}",
         "2025-03-27 08:07:57.835065",
         "/mnt/datalake/output/Avroanswer.parquet"
        ],
        [
         null,
         "[]",
         null,
         null,
         null,
         null,
         "\n                CREATE TABLE IF NOT EXISTS employee_table5 (\n                    id INT,\n                    name STRING,\n                    department STRING,\n                    salary INT\n                )\n                USING DELTA;\n                ",
         "2025-04-01 09:13:28.039220",
         null,
         null,
         null,
         "[employee_table5]"
        ],
        [
         null,
         "[]",
         null,
         null,
         null,
         null,
         "\n                CREATE TABLE IF NOT EXISTS employee_table4 (\n                    id INT,\n                    name STRING,\n                    department STRING,\n                    salary INT\n                )\n                USING DELTA;\n                ",
         "2025-03-26 09:20:38.031664",
         null,
         null,
         null,
         "[employee_table4]"
        ],
        [
         null,
         "[]",
         null,
         null,
         null,
         null,
         "\n                INSERT INTO employee_table4 VALUES\n                (1, 'John Doe', 'Engineering', 1000),\n                (2, 'Jane Smith', 'Marketing', 1200),\n                (3, 'Alice Johnson', 'Finance', 1400),\n                (4, 'Bob Brown', 'Engineering', 1300),\n                (5, 'Charlie White', 'Sales', 1100);\n                ",
         "2025-03-26 09:20:44.278997",
         null,
         null,
         null,
         "[employee_table4]"
        ],
        [
         null,
         "[]",
         null,
         null,
         null,
         null,
         "\n                CREATE TABLE IF NOT EXISTS employee_table4 (\n                    id INT,\n                    name STRING,\n                    department STRING,\n                    salary INT\n                )\n                USING DELTA;\n                ",
         "2025-03-27 08:03:19.809104",
         null,
         null,
         null,
         "[employee_table4]"
        ],
        [
         null,
         "[]",
         null,
         null,
         null,
         null,
         "\n                CREATE TABLE IF NOT EXISTS employee_table5 (\n                    id INT,\n                    name STRING,\n                    department STRING,\n                    salary INT\n                )\n                USING DELTA;\n                ",
         "2025-03-27 08:04:49.613385",
         null,
         null,
         null,
         "[employee_table5]"
        ],
        [
         null,
         "[]",
         null,
         null,
         null,
         null,
         "\n                INSERT INTO employee_table5 VALUES\n                (1, 'John Doe', 'Engineering', 1000),\n                (2, 'Jane Smith', 'Marketing', 1200),\n                (3, 'Alice Johnson', 'Finance', 1400),\n                (4, 'Bob Brown', 'Engineering', 1300),\n                (5, 'Charlie White', 'Sales', 1100);\n                ",
         "2025-03-27 08:05:01.238966",
         null,
         null,
         null,
         "[employee_table5]"
        ],
        [
         null,
         "[employee_table5]",
         null,
         null,
         null,
         null,
         "\n                    SELECT * FROM employee_table5;\n                   ",
         "2025-03-27 08:05:12.680028",
         null,
         null,
         null,
         "[unknown]"
        ],
        [
         null,
         "[]",
         null,
         null,
         null,
         null,
         "\n                INSERT INTO employee_table4 VALUES\n                (1, 'John Doe', 'Engineering', 1000),\n                (2, 'Jane Smith', 'Marketing', 1200),\n                (3, 'Alice Johnson', 'Finance', 1400),\n                (4, 'Bob Brown', 'Engineering', 1300),\n                (5, 'Charlie White', 'Sales', 1100);\n                ",
         "2025-03-26 10:09:31.829305",
         null,
         null,
         null,
         "[employee_table4]"
        ],
        [
         null,
         "[employee_table4]",
         null,
         null,
         null,
         null,
         "\n                    SELECT * FROM employee_table4;\n                   ",
         "2025-03-26 10:09:36.290130",
         null,
         null,
         null,
         "[unknown]"
        ],
        [
         null,
         "[employee_table4]",
         null,
         null,
         null,
         null,
         "\n                    SELECT * FROM employee_table4;\n                   ",
         "2025-03-26 09:20:49.919760",
         null,
         null,
         null,
         "[unknown]"
        ],
        [
         null,
         "[]",
         null,
         null,
         null,
         null,
         "\n                CREATE TABLE IF NOT EXISTS employee_table4 (\n                    id INT,\n                    name STRING,\n                    department STRING,\n                    salary INT\n                )\n                USING DELTA;\n                ",
         "2025-03-26 10:09:26.634296",
         null,
         null,
         null,
         "[employee_table4]"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "uuid",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "source",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "file_format",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "read_parameters",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "read_timestamp",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "transformation_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "modification_details",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "modification_timestamp",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "write_action",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "write_details",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "write_timestamp",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "destination",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Combined File\n",
    "combine_sql_file()\n",
    "lineage_logs = spark.read.format(\"delta\").load(\"/mnt/datalake/Pipeline1/lineage/lineage_logs/\")\n",
    "\n",
    "lineage_logs.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": ""
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Automated Data Lineage Demo 1 Ready",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}